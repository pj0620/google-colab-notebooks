{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pj0620/google-colab-notebooks/blob/main/Minesweeper_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from scipy.signal import convolve2d\n",
        "%pip install stable-baselines3[extra]\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "\n",
        "!apt-get install swig cmake ffmpeg\n",
        "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LZv1Xe1yddn4",
        "outputId": "387fb2f0-c4c2-45cf-f9f5-98a8b0107ff0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.3)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (10.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=d371d435ce5edfcbde0bba34849f02ac9c03e62ae8ff83b12930d9a8fd5014ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.3.0\n",
            "    Uninstalling Shimmy-1.3.0:\n",
            "      Successfully uninstalled Shimmy-1.3.0\n",
            "Successfully installed shimmy-0.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "shimmy"
                ]
              },
              "id": "10765a0ff50c4a3bbc216677dbe91e7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.8.30)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: autorom\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.6.1\n",
            "    Uninstalling AutoROM-0.6.1:\n",
            "      Successfully uninstalled AutoROM-0.6.1\n",
            "Successfully installed autorom-0.4.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (8,358 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting git+https://github.com/DLR-RM/rl-baselines3-zoo\n",
            "  Cloning https://github.com/DLR-RM/rl-baselines3-zoo to /tmp/pip-req-build-v8ij4196\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/DLR-RM/rl-baselines3-zoo /tmp/pip-req-build-v8ij4196\n",
            "  Resolved https://github.com/DLR-RM/rl-baselines3-zoo to commit 6409c410d0c613dd6efa4582bf75a9fd9658f5a4\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sb3-contrib<3.0,>=2.4.0a10 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading sb3_contrib-2.4.0a10-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: gymnasium~=0.29.1 in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (0.29.1)\n",
            "Collecting huggingface-sb3<4.0,>=3.0 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (4.66.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (13.9.3)\n",
            "Collecting optuna>=3.0 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (6.0.2)\n",
            "Collecting pytablewriter~=1.2 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (0.0.4)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (0.24.7)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (1.1.3)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.0->rl_zoo3==2.4.0a10) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.0->rl_zoo3==2.4.0a10) (2.0.36)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter~=1.2->rl_zoo3==2.4.0a10) (75.1.0)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting stable-baselines3<3.0,>=2.4.0a6 (from sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10)\n",
            "  Downloading stable_baselines3-2.4.0a10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->rl_zoo3==2.4.0a10) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->rl_zoo3==2.4.0a10) (2.18.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2.32.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->rl_zoo3==2.4.0a10) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (5.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.0->rl_zoo3==2.4.0a10) (3.1.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.0->typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna>=3.0->rl_zoo3==2.4.0a10) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.2.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2024.8.30)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sb3_contrib-2.4.0a10-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading stable_baselines3-2.4.0a10-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.5/183.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rl_zoo3\n",
            "  Building wheel for rl_zoo3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rl_zoo3: filename=rl_zoo3-2.4.0a10-py3-none-any.whl size=76997 sha256=424b1d78b29a869ce6a3027ac45b2978ddc220f64fb5336b99ce470bf1b425b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7pcaedtv/wheels/5f/9b/c0/8af7ab740f894e9d66c7707eb4aa58087c9a71dd262e7e6174\n",
            "Successfully built rl_zoo3\n",
            "Installing collected packages: tcolorpy, pathvalidate, mbstrdecoder, Mako, colorlog, typepy, alembic, stable-baselines3, optuna, huggingface-sb3, sb3-contrib, DataProperty, tabledata, pytablewriter, rl_zoo3\n",
            "  Attempting uninstall: stable-baselines3\n",
            "    Found existing installation: stable_baselines3 2.3.2\n",
            "    Uninstalling stable_baselines3-2.3.2:\n",
            "      Successfully uninstalled stable_baselines3-2.3.2\n",
            "Successfully installed DataProperty-1.0.1 Mako-1.3.6 alembic-1.13.3 colorlog-6.9.0 huggingface-sb3-3.0 mbstrdecoder-1.1.3 optuna-4.0.0 pathvalidate-3.2.1 pytablewriter-1.2.0 rl_zoo3-2.4.0a10 sb3-contrib-2.4.0a10 stable-baselines3-2.4.0a10 tabledata-1.3.3 tcolorpy-0.1.6 typepy-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter configurations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BjNKEnmJ44KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a file named `dql.yml` with the following contents\n",
        "\n",
        "```\n",
        "Minesweeper-v1:\n",
        "  frame_stack: 1\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "```\n"
      ],
      "metadata": {
        "id": "mtC5i_SBwgyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom gymnasium env"
      ],
      "metadata": {
        "id": "qnhdrAKdlZdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from skimage.transform import resize\n",
        "\n",
        "CLICK_BOMB = \"click-bomb\"\n",
        "GAME_WIN = \"game-win\"\n",
        "GAME_LOSE = \"game-lose\"\n",
        "CLICK_VISIBLE = \"click-visible\"\n",
        "CLICK_GUESS = \"click-guess\"\n",
        "CLICK_VALID = \"click-valid\"\n",
        "\n",
        "class MinesweeperEnvironment(gym.Env):\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "  def __init__(self, board_size=9, total_bombs=8, render_mode=\"console\", end_on_bomb=False):\n",
        "    super().__init__()\n",
        "    self.render_mode = render_mode\n",
        "    self.board_size = board_size\n",
        "    self.total_bombs = total_bombs\n",
        "    self.end_on_bomb = end_on_bomb\n",
        "\n",
        "    self.visible = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "    self.set_bombs()\n",
        "    self.set_values()\n",
        "\n",
        "    # Define observation space\n",
        "    # self.observation_space = spaces.Dict({\n",
        "    #   \"visible_vals\": spaces.Box(low=0, high=8, shape=(self.board_size, self.board_size), dtype=np.int8),\n",
        "    #   \"visible\": spaces.Box(low=0, high=1, shape=(self.board_size, self.board_size), dtype=np.int8)\n",
        "    # })\n",
        "\n",
        "    # self.observation_space = spaces.Box(\n",
        "    #     low=0, high=8, shape=(2 * self.board_size**2,), dtype=np.int8\n",
        "    # )\n",
        "\n",
        "    self.observation_space = spaces.Box(\n",
        "        low=-1, high=1, shape=(self.board_size, self.board_size, 2)\n",
        "    )\n",
        "\n",
        "    # Define action space\n",
        "    self.action_space = spaces.Discrete(self.board_size**2)\n",
        "\n",
        "    self.last_action = -1\n",
        "\n",
        "  def set_bombs(self):\n",
        "    random.seed(10)\n",
        "    self.bombs = np.zeros(shape=(self.board_size, self.board_size),  dtype=np.uint8)\n",
        "    placed_bombs = 0\n",
        "    while placed_bombs < self.total_bombs:\n",
        "      i = random.randint(0, self.board_size-1)\n",
        "      j = random.randint(0, self.board_size-1)\n",
        "\n",
        "      if self.bombs[i][j] == 0:\n",
        "        self.bombs[i][j] = 1\n",
        "        placed_bombs += 1\n",
        "\n",
        "  def set_values(self):\n",
        "    KERNAL = np.ones((3, 3))\n",
        "    self.vals = convolve2d(self.bombs, KERNAL, mode='same').astype(np.uint8)\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    # Reset the environment to an initial state\n",
        "    self.visible = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "\n",
        "    # same everytime\n",
        "    # self.set_bombs()\n",
        "    # self.set_values()\n",
        "    self.last_action = -1\n",
        "\n",
        "    return self.get_state(), {}\n",
        "\n",
        "  def propogate(self, x: int, y: int):\n",
        "    if self.bombs[x][y] == 1 or self.visible[x][y] == 1:\n",
        "      return\n",
        "\n",
        "    self.visible[x][y] = 1\n",
        "    if self.vals[x][y] == 0:\n",
        "      for x_k in [x - 1, x, x + 1]:\n",
        "        for y_k in [y - 1, y, y + 1]:\n",
        "          if (x_k, y_k) == (x, y):\n",
        "            continue\n",
        "\n",
        "          if x_k < 0 or x_k > self.board_size - 1:\n",
        "            continue\n",
        "\n",
        "          if y_k < 0 or y_k > self.board_size - 1:\n",
        "            continue\n",
        "\n",
        "          self.propogate(x_k, y_k)\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # Implement the logic for taking a step in the environment\n",
        "    x = action // self.board_size\n",
        "    y = action % self.board_size\n",
        "\n",
        "    start_visible_cells = np.sum(self.visible)\n",
        "\n",
        "    if self.visible[x][y] == 1:\n",
        "      start_teminated = bool((self.board_size**2 - start_visible_cells) == self.total_bombs)\n",
        "      return self.get_state(), -0.5, start_teminated, False, {\"effect\": CLICK_VISIBLE}\n",
        "      # return self.get_state(), -1, True, True, {}\n",
        "    elif self.bombs[x][y] == 1:\n",
        "      if self.end_on_bomb:\n",
        "        return self.get_state(), -1, True, True, {\"effect\": CLICK_BOMB}\n",
        "      else:\n",
        "        return self.get_state(), -1, False, False, {\"effect\": CLICK_BOMB}\n",
        "\n",
        "    next_to_zero = False\n",
        "    # if start_visible_cells > 1:\n",
        "    for x_k in [x - 1, x, x + 1]:\n",
        "      for y_k in [y - 1, y, y + 1]:\n",
        "        if (x_k, y_k) == (x, y):\n",
        "          continue\n",
        "\n",
        "        if x_k < 0 or x_k > self.board_size - 1:\n",
        "          continue\n",
        "\n",
        "        if y_k < 0 or y_k > self.board_size - 1:\n",
        "          continue\n",
        "\n",
        "        if self.visible[x_k][y_k] == 1:\n",
        "          next_to_zero = True\n",
        "          break\n",
        "    # else:\n",
        "    #   next_to_zero = True\n",
        "\n",
        "    self.last_action = action\n",
        "\n",
        "    if self.vals[x][y] == 0:\n",
        "      self.propogate(x, y)\n",
        "\n",
        "    self.visible[x][y] = 1\n",
        "    end_visible_cells = np.sum(self.visible)\n",
        "    teminated = bool((self.board_size**2 - end_visible_cells) == self.total_bombs)\n",
        "\n",
        "    if teminated:\n",
        "      return self.get_state(), 1, teminated, False, {\"effect\": GAME_WIN}\n",
        "    else:\n",
        "      # print(f\"start_visible_cells: {start_visible_cells}\")\n",
        "      # print(f\"end_visible_cells: {end_visible_cells}\")\n",
        "      # reward = 2 * (float(end_visible_cells) - float(start_visible_cells)) / (self.board_size**2)\n",
        "      reward = 0.3 if next_to_zero else -0.3\n",
        "      info = CLICK_VALID if next_to_zero else CLICK_GUESS\n",
        "      return self.get_state(), reward, teminated, False, {\"effect\": info}\n",
        "\n",
        "  def get_state(self):\n",
        "    visible_vals = self.visible * self.vals\n",
        "    visible_vals = (visible_vals.astype(np.float32) - 4) / 4\n",
        "    visible_scaled = 2 * (self.visible.astype(np.float32) - 0.5)\n",
        "    return np.stack([visible_vals, visible_scaled], axis=2).astype(np.float32)\n",
        "\n",
        "  def render(self, mode=\"console\"):\n",
        "    if self.render_mode != \"console\":\n",
        "        raise NotImplementedError(\"Render mode not supported.\")\n",
        "\n",
        "    # Print the current visible board state\n",
        "    cells_left = int(self.board_size**2 - np.sum(self.visible))\n",
        "    print(f\"Current Board: {cells_left - self.total_bombs} Cells Left\")\n",
        "    print(\"# \" + \" \".join(str(i) for i in range(self.board_size)))\n",
        "    for i in range(self.board_size):\n",
        "        print(f\"{i} \", end=\"\")\n",
        "        row = \"\"\n",
        "        for j in range(self.board_size):\n",
        "            if self.visible[i][j] == 1:\n",
        "                # If the cell is visible, show its value (number of adjacent bombs)\n",
        "                if self.bombs[i][j] == 1:\n",
        "                  row += f\"B \"\n",
        "                else:\n",
        "                  row += f\"{self.vals[i][j]} \"\n",
        "            else:\n",
        "                # If the cell is hidden, show an asterisk\n",
        "                row += \"* \"\n",
        "        print(row)\n",
        "    # print(\"\\n\")\n",
        "\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "env = MinesweeperEnvironment()\n",
        "\n",
        "# Testing env\n",
        "check_env(env)\n",
        "print(\"starting game\")\n",
        "for round in range(1):\n",
        "  obs, info = env.reset()\n",
        "  for _ in range(10):\n",
        "      # Random action\n",
        "      action = env.action_space.sample()\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "      if terminated:\n",
        "          obs, info = env.reset()\n",
        "      env.render()\n",
        "      reward_pos = f\"{reward * (env.board_size ** 2)} / {env.board_size ** 2}\"\n",
        "      print(f\"action: {action} -> {(action // env.board_size, action % env.board_size)}\")\n",
        "      # print(f\"reward: {reward if reward <= 0 else reward_pos}\")\n",
        "      print(f\"reward: {reward}\")\n",
        "      print(\"\\n\")\n",
        "  env.reset()"
      ],
      "metadata": {
        "id": "k1b3Xo2biryW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825a7b1e-263d-4dd2-8a38-6914f7e9512c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting game\n",
            "Current Board: 73 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "action: 18 -> (2, 0)\n",
            "reward: -1\n",
            "\n",
            "\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 1 * * * * * * * * \n",
            "action: 72 -> (8, 0)\n",
            "reward: -0.3\n",
            "\n",
            "\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "action: 13 -> (1, 4)\n",
            "reward: -0.3\n",
            "\n",
            "\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "action: 50 -> (5, 5)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 74 -> (8, 2)\n",
            "reward: -0.3\n",
            "\n",
            "\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 24 -> (2, 6)\n",
            "reward: 0.3\n",
            "\n",
            "\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 41 -> (4, 5)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 65 -> (7, 2)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 65 -> (7, 2)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "action: 15 -> (1, 6)\n",
            "reward: 0.3\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:54: UserWarning: It seems that your observation  is an image but its `dtype` is (float32) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:62: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:75: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play minesweeper"
      ],
      "metadata": {
        "id": "grsj8j6_ddhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = MinesweeperEnvironment(end_on_bomb=True)\n",
        "\n",
        "terminated = False\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "while not terminated:\n",
        "  actions_str = input(\"enter row then column with a space(\\\"X Y\\\"):\").split(\" \")\n",
        "  actions_int = [int(x) for x in actions_str]\n",
        "  action = actions_int[0] + actions_int[1] * env.board_size\n",
        "  obs, reward, terminated, truncated, info = env.step(action)\n",
        "  print(f\"reward: {reward}\")\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1NlklL2dctR",
        "outputId": "75d938e5-b622-4962-add8-b4f09d5c0808"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Board: 73 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 3\n",
            "reward: -0.3\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 4\n",
            "reward: 0.3\n",
            "Current Board: 71 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 1 * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):5 4\n",
            "reward: -0.3\n",
            "Current Board: 70 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 1 * * * * 1 * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):4 5\n",
            "reward: 0.3\n",
            "Current Board: 24 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * * * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 3\n",
            "reward: 0.3\n",
            "Current Board: 23 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 3\n",
            "reward: 0.3\n",
            "Current Board: 22 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 2\n",
            "reward: 0.3\n",
            "Current Board: 21 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 2\n",
            "reward: 0.3\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 2\n",
            "reward: 0.3\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 1\n",
            "reward: 0.3\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 0\n",
            "reward: 0.3\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 1\n",
            "reward: 0.3\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 0\n",
            "reward: 0.3\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 5\n",
            "reward: 0.3\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 6\n",
            "reward: 0.3\n",
            "Current Board: 12 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):1 6\n",
            "reward: 0.3\n",
            "Current Board: 11 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):2 7\n",
            "reward: 0.3\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 8\n",
            "reward: 0.3\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "enter row then column with a space(\"X Y\"):4 8\n",
            "reward: 0.3\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 * * * * \n",
            "enter row then column with a space(\"X Y\"):5 8\n",
            "reward: 0.3\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "enter row then column with a space(\"X Y\"):6 8\n",
            "reward: 0.3\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * * \n",
            "enter row then column with a space(\"X Y\"):8 8\n",
            "reward: 1\n",
            "Current Board: 0 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2U3JfCJ-fh7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "K-bx0rrue1G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "# 18418\n",
        "log(0.01) / log(0.99975)\n",
        "\n",
        "18418 / 100_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcfUwu7F5yEP",
        "outputId": "7e6fecd6-0590-4559-9c97-c088b4527419"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18418"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, features_dim=256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[2]  # Should be 2 for (9, 9, 2) input\n",
        "\n",
        "        # Define a custom CNN architecture\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the CNN dynamically\n",
        "        with th.no_grad():\n",
        "            sample_input = th.as_tensor(observation_space.sample()[None]).float().permute(0, 3, 1, 2)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "            print(\"Flattened output size after CNN:\", n_flatten)  # Debugging statement\n",
        "\n",
        "        # Define fully connected layers using the computed flattened size\n",
        "        # self.linear = nn.Sequential(\n",
        "        #     nn.Linear(n_flatten, 256),  # Use dynamically calculated n_flatten\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, 128),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, features_dim)  # Output size of features_dim (256)\n",
        "        # )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        observations = observations.permute(0, 3, 1, 2)  # Rearrange dimensions for Conv2d\n",
        "        cnn_output = self.cnn(observations)\n",
        "        return self.linear(cnn_output)\n"
      ],
      "metadata": {
        "id": "Mo6iNjTC9dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746bb4bf-2453-46c3-b268-b9f5ef45b677"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "\n",
        "vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "\n",
        "# vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "# vec_env = VecTransposeImage(vec_env)  # Transpose to [batch_size, channels, height, width]\n",
        "\n",
        "# Optimized hyperparameters for DQN\n",
        "learning_rate = 1e-4           # Learning rate for weight updates\n",
        "buffer_size = 300_000            # Replay buffer size\n",
        "learning_starts = 1000         # Steps before learning begins\n",
        "batch_size = 64                # Number of samples per training update\n",
        "train_freq = (4, 'step')    # Frequency of training updates\n",
        "# train_freq = (1, 'episode')\n",
        "target_update_interval = 750   # Steps between target network updates\n",
        "exploration_fraction = 0.25    # Fraction of total timesteps for epsilon decay\n",
        "exploration_final_eps = 0.01   # Final epsilon value after decay\n",
        "gamma = 0.99                    # Discount factor for future rewards\n",
        "\n",
        "#  0.99975 ** x = 0.01 => x = log(0.01) / log(0.99975)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        "    features_extractor_kwargs=dict(features_dim=256)  # Output size of the final layer\n",
        ")\n",
        "\n",
        "# Instantiate DQN with improved hyperparameters\n",
        "model = DQN(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    learning_rate=learning_rate,\n",
        "    buffer_size=buffer_size,\n",
        "    learning_starts=learning_starts,\n",
        "    batch_size=batch_size,\n",
        "    train_freq=train_freq,\n",
        "    target_update_interval=target_update_interval,\n",
        "    exploration_fraction=exploration_fraction,\n",
        "    exploration_final_eps=exploration_final_eps,\n",
        "    gamma=gamma,\n",
        "    policy_kwargs=policy_kwargs,  # Use custom network\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=300_000)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cby_Zfh7AA74",
        "outputId": "4aedc4c2-e232-462e-8744-b037cac31daa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Flattened output size after CNN: 2592\n",
            "Flattened output size after CNN: 2592\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 284      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.985    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 4079     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1138     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0908   |\n",
            "|    n_updates        | 34       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -143     |\n",
            "|    exploration_rate | 0.969    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 1382     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 2324     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0283   |\n",
            "|    n_updates        | 330      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 274      |\n",
            "|    ep_rew_mean      | -135     |\n",
            "|    exploration_rate | 0.957    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 1094     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 3284     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0132   |\n",
            "|    n_updates        | 570      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 262      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.945    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 936      |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 4187     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0177   |\n",
            "|    n_updates        | 796      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 251      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.934    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 886      |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 5029     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00983  |\n",
            "|    n_updates        | 1007     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 244      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.923    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 882      |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 5856     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00771  |\n",
            "|    n_updates        | 1213     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 252      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.907    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 7049     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0151   |\n",
            "|    n_updates        | 1512     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 261      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.89     |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 860      |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 8358     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00731  |\n",
            "|    n_updates        | 1839     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 251      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.881    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 855      |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 9027     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0139   |\n",
            "|    n_updates        | 2006     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 254      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.866    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 848      |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 10162    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00965  |\n",
            "|    n_updates        | 2290     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 248      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.856    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 843      |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 10909    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00385  |\n",
            "|    n_updates        | 2477     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 247      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.844    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 838      |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 11855    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00799  |\n",
            "|    n_updates        | 2713     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 238      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.836    |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 835      |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 12395    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0106   |\n",
            "|    n_updates        | 2848     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 233      |\n",
            "|    ep_rew_mean      | -112     |\n",
            "|    exploration_rate | 0.828    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 823      |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 13030    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00393  |\n",
            "|    n_updates        | 3007     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 228      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.819    |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 805      |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total_timesteps  | 13685    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00765  |\n",
            "|    n_updates        | 3171     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 225      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 790      |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 14378    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.011    |\n",
            "|    n_updates        | 3344     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 224      |\n",
            "|    ep_rew_mean      | -107     |\n",
            "|    exploration_rate | 0.799    |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 788      |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 15221    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0161   |\n",
            "|    n_updates        | 3555     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 224      |\n",
            "|    ep_rew_mean      | -107     |\n",
            "|    exploration_rate | 0.787    |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 786      |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 16106    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0914   |\n",
            "|    n_updates        | 3776     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 220      |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.78     |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 784      |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 16701    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00882  |\n",
            "|    n_updates        | 3925     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 219      |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.768    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 782      |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total_timesteps  | 17559    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0065   |\n",
            "|    n_updates        | 4139     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 223      |\n",
            "|    ep_rew_mean      | -107     |\n",
            "|    exploration_rate | 0.753    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 780      |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 18749    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.012    |\n",
            "|    n_updates        | 4437     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 221      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.743    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 778      |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 19445    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00998  |\n",
            "|    n_updates        | 4611     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 217      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.736    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 776      |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 20003    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 4750     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 217      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.725    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 774      |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 20807    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00573  |\n",
            "|    n_updates        | 4951     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 215      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.717    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 772      |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 21470    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0117   |\n",
            "|    n_updates        | 5117     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 210      |\n",
            "|    ep_rew_mean      | -99.2    |\n",
            "|    exploration_rate | 0.708    |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 765      |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 22091    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00476  |\n",
            "|    n_updates        | 5272     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 206      |\n",
            "|    ep_rew_mean      | -97.2    |\n",
            "|    exploration_rate | 0.697    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 752      |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 22918    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 5479     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 204      |\n",
            "|    ep_rew_mean      | -95.7    |\n",
            "|    exploration_rate | 0.688    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 747      |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 23650    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.014    |\n",
            "|    n_updates        | 5662     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 202      |\n",
            "|    ep_rew_mean      | -94.6    |\n",
            "|    exploration_rate | 0.678    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 745      |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 24395    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0053   |\n",
            "|    n_updates        | 5848     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 203      |\n",
            "|    ep_rew_mean      | -94.7    |\n",
            "|    exploration_rate | 0.666    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 744      |\n",
            "|    time_elapsed     | 33       |\n",
            "|    total_timesteps  | 25283    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0426   |\n",
            "|    n_updates        | 6070     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 202      |\n",
            "|    ep_rew_mean      | -94.6    |\n",
            "|    exploration_rate | 0.655    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 743      |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 26105    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0213   |\n",
            "|    n_updates        | 6276     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 198      |\n",
            "|    ep_rew_mean      | -91.7    |\n",
            "|    exploration_rate | 0.646    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 741      |\n",
            "|    time_elapsed     | 36       |\n",
            "|    total_timesteps  | 26810    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.133    |\n",
            "|    n_updates        | 6452     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 191      |\n",
            "|    ep_rew_mean      | -88      |\n",
            "|    exploration_rate | 0.637    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 740      |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 27464    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 6615     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 195      |\n",
            "|    ep_rew_mean      | -90.2    |\n",
            "|    exploration_rate | 0.623    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 738      |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 28556    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00941  |\n",
            "|    n_updates        | 6888     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 190      |\n",
            "|    ep_rew_mean      | -87      |\n",
            "|    exploration_rate | 0.616    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 737      |\n",
            "|    time_elapsed     | 39       |\n",
            "|    total_timesteps  | 29120    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00691  |\n",
            "|    n_updates        | 7029     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 194      |\n",
            "|    ep_rew_mean      | -89.6    |\n",
            "|    exploration_rate | 0.599    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 733      |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total_timesteps  | 30353    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 7338     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 194      |\n",
            "|    ep_rew_mean      | -89.1    |\n",
            "|    exploration_rate | 0.588    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 724      |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 31226    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00774  |\n",
            "|    n_updates        | 7556     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 196      |\n",
            "|    ep_rew_mean      | -90.4    |\n",
            "|    exploration_rate | 0.577    |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 720      |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 32018    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00885  |\n",
            "|    n_updates        | 7754     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 198      |\n",
            "|    ep_rew_mean      | -91.4    |\n",
            "|    exploration_rate | 0.566    |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 713      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 32878    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.311    |\n",
            "|    n_updates        | 7969     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 202      |\n",
            "|    ep_rew_mean      | -93.2    |\n",
            "|    exploration_rate | 0.553    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 707      |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 33881    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.17     |\n",
            "|    n_updates        | 8220     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 206      |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.538    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 705      |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 34978    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.016    |\n",
            "|    n_updates        | 8494     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 203      |\n",
            "|    ep_rew_mean      | -93.5    |\n",
            "|    exploration_rate | 0.531    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 703      |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 35567    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00503  |\n",
            "|    n_updates        | 8641     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 203      |\n",
            "|    ep_rew_mean      | -92.8    |\n",
            "|    exploration_rate | 0.52     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 702      |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 36366    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00629  |\n",
            "|    n_updates        | 8841     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 205      |\n",
            "|    ep_rew_mean      | -94      |\n",
            "|    exploration_rate | 0.509    |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 701      |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 37221    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0293   |\n",
            "|    n_updates        | 9055     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 204      |\n",
            "|    ep_rew_mean      | -93.1    |\n",
            "|    exploration_rate | 0.499    |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 698      |\n",
            "|    time_elapsed     | 54       |\n",
            "|    total_timesteps  | 37925    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0117   |\n",
            "|    n_updates        | 9231     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 204      |\n",
            "|    ep_rew_mean      | -92.9    |\n",
            "|    exploration_rate | 0.484    |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 688      |\n",
            "|    time_elapsed     | 56       |\n",
            "|    total_timesteps  | 39111    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00513  |\n",
            "|    n_updates        | 9527     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 203      |\n",
            "|    ep_rew_mean      | -92.6    |\n",
            "|    exploration_rate | 0.475    |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 687      |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 39769    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0162   |\n",
            "|    n_updates        | 9692     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 205      |\n",
            "|    ep_rew_mean      | -93.4    |\n",
            "|    exploration_rate | 0.465    |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 686      |\n",
            "|    time_elapsed     | 59       |\n",
            "|    total_timesteps  | 40517    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.207    |\n",
            "|    n_updates        | 9879     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 206      |\n",
            "|    ep_rew_mean      | -93.7    |\n",
            "|    exploration_rate | 0.454    |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 685      |\n",
            "|    time_elapsed     | 60       |\n",
            "|    total_timesteps  | 41398    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0131   |\n",
            "|    n_updates        | 10099    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 208      |\n",
            "|    ep_rew_mean      | -94.6    |\n",
            "|    exploration_rate | 0.442    |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 683      |\n",
            "|    time_elapsed     | 61       |\n",
            "|    total_timesteps  | 42256    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00644  |\n",
            "|    n_updates        | 10313    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 210      |\n",
            "|    ep_rew_mean      | -95.4    |\n",
            "|    exploration_rate | 0.432    |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 682      |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 43061    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0079   |\n",
            "|    n_updates        | 10515    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 209      |\n",
            "|    ep_rew_mean      | -95.1    |\n",
            "|    exploration_rate | 0.421    |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 681      |\n",
            "|    time_elapsed     | 64       |\n",
            "|    total_timesteps  | 43837    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0081   |\n",
            "|    n_updates        | 10709    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 211      |\n",
            "|    ep_rew_mean      | -96.1    |\n",
            "|    exploration_rate | 0.409    |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 680      |\n",
            "|    time_elapsed     | 65       |\n",
            "|    total_timesteps  | 44797    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00658  |\n",
            "|    n_updates        | 10949    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 211      |\n",
            "|    ep_rew_mean      | -95.7    |\n",
            "|    exploration_rate | 0.4      |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 677      |\n",
            "|    time_elapsed     | 67       |\n",
            "|    total_timesteps  | 45477    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0101   |\n",
            "|    n_updates        | 11119    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 212      |\n",
            "|    ep_rew_mean      | -96.5    |\n",
            "|    exploration_rate | 0.386    |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 669      |\n",
            "|    time_elapsed     | 69       |\n",
            "|    total_timesteps  | 46531    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.215    |\n",
            "|    n_updates        | 11382    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 212      |\n",
            "|    ep_rew_mean      | -96.3    |\n",
            "|    exploration_rate | 0.375    |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 668      |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 47354    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.209    |\n",
            "|    n_updates        | 11588    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 213      |\n",
            "|    ep_rew_mean      | -96.4    |\n",
            "|    exploration_rate | 0.365    |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 667      |\n",
            "|    time_elapsed     | 72       |\n",
            "|    total_timesteps  | 48106    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00838  |\n",
            "|    n_updates        | 11776    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 216      |\n",
            "|    ep_rew_mean      | -98      |\n",
            "|    exploration_rate | 0.352    |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 665      |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 49061    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00683  |\n",
            "|    n_updates        | 12015    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 217      |\n",
            "|    ep_rew_mean      | -98.3    |\n",
            "|    exploration_rate | 0.336    |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 664      |\n",
            "|    time_elapsed     | 75       |\n",
            "|    total_timesteps  | 50285    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00611  |\n",
            "|    n_updates        | 12321    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 224      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.32     |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 662      |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 51506    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.217    |\n",
            "|    n_updates        | 12626    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 221      |\n",
            "|    ep_rew_mean      | -99.9    |\n",
            "|    exploration_rate | 0.308    |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 661      |\n",
            "|    time_elapsed     | 79       |\n",
            "|    total_timesteps  | 52438    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0188   |\n",
            "|    n_updates        | 12859    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 223      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.293    |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 654      |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 53553    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00928  |\n",
            "|    n_updates        | 13138    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 226      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.279    |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 652      |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 54585    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0078   |\n",
            "|    n_updates        | 13396    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 226      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.267    |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 650      |\n",
            "|    time_elapsed     | 85       |\n",
            "|    total_timesteps  | 55522    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00791  |\n",
            "|    n_updates        | 13630    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 230      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.249    |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 87       |\n",
            "|    total_timesteps  | 56885    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.231    |\n",
            "|    n_updates        | 13971    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 235      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.228    |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 645      |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total_timesteps  | 58461    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00833  |\n",
            "|    n_updates        | 14365    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 242      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.211    |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 642      |\n",
            "|    time_elapsed     | 93       |\n",
            "|    total_timesteps  | 59750    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00525  |\n",
            "|    n_updates        | 14687    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 245      |\n",
            "|    ep_rew_mean      | -112     |\n",
            "|    exploration_rate | 0.196    |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 60902    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0136   |\n",
            "|    n_updates        | 14975    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 250      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.178    |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 634      |\n",
            "|    time_elapsed     | 98       |\n",
            "|    total_timesteps  | 62265    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0231   |\n",
            "|    n_updates        | 15316    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 253      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.165    |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 633      |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 63270    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0122   |\n",
            "|    n_updates        | 15567    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 255      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.147    |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 102      |\n",
            "|    total_timesteps  | 64659    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00854  |\n",
            "|    n_updates        | 15914    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 263      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.127    |\n",
            "| time/               |          |\n",
            "|    episodes         | 288      |\n",
            "|    fps              | 629      |\n",
            "|    time_elapsed     | 105      |\n",
            "|    total_timesteps  | 66115    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00664  |\n",
            "|    n_updates        | 16278    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 265      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.115    |\n",
            "| time/               |          |\n",
            "|    episodes         | 292      |\n",
            "|    fps              | 624      |\n",
            "|    time_elapsed     | 107      |\n",
            "|    total_timesteps  | 67049    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00701  |\n",
            "|    n_updates        | 16512    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 274      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.0921   |\n",
            "| time/               |          |\n",
            "|    episodes         | 296      |\n",
            "|    fps              | 620      |\n",
            "|    time_elapsed     | 110      |\n",
            "|    total_timesteps  | 68783    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00395  |\n",
            "|    n_updates        | 16945    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 282      |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.0694   |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 618      |\n",
            "|    time_elapsed     | 114      |\n",
            "|    total_timesteps  | 70501    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0231   |\n",
            "|    n_updates        | 17375    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 292      |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.0466   |\n",
            "| time/               |          |\n",
            "|    episodes         | 304      |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 72225    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.287    |\n",
            "|    n_updates        | 17806    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 300      |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.0248   |\n",
            "| time/               |          |\n",
            "|    episodes         | 308      |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 121      |\n",
            "|    total_timesteps  | 73877    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00691  |\n",
            "|    n_updates        | 18219    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 323      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 312      |\n",
            "|    fps              | 604      |\n",
            "|    time_elapsed     | 127      |\n",
            "|    total_timesteps  | 77113    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.565    |\n",
            "|    n_updates        | 19028    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 356      |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 316      |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 81117    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.019    |\n",
            "|    n_updates        | 20029    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 320      |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 83921    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 20730    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 324      |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 146      |\n",
            "|    total_timesteps  | 86225    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0129   |\n",
            "|    n_updates        | 21306    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 328      |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 150      |\n",
            "|    total_timesteps  | 88285    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00936  |\n",
            "|    n_updates        | 21821    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 421      |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 332      |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 91181    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00581  |\n",
            "|    n_updates        | 22545    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 443      |\n",
            "|    ep_rew_mean      | -209     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 336      |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 94613    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00583  |\n",
            "|    n_updates        | 23403    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 466      |\n",
            "|    ep_rew_mean      | -220     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 170      |\n",
            "|    total_timesteps  | 98081    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00802  |\n",
            "|    n_updates        | 24270    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 476      |\n",
            "|    ep_rew_mean      | -225     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 344      |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 174      |\n",
            "|    total_timesteps  | 100049   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0132   |\n",
            "|    n_updates        | 24762    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 494      |\n",
            "|    ep_rew_mean      | -234     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 348      |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 180      |\n",
            "|    total_timesteps  | 102965   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0104   |\n",
            "|    n_updates        | 25491    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 510      |\n",
            "|    ep_rew_mean      | -242     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 352      |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 186      |\n",
            "|    total_timesteps  | 105573   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00825  |\n",
            "|    n_updates        | 26143    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 524      |\n",
            "|    ep_rew_mean      | -249     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 356      |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 190      |\n",
            "|    total_timesteps  | 107961   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0187   |\n",
            "|    n_updates        | 26740    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 536      |\n",
            "|    ep_rew_mean      | -255     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 196      |\n",
            "|    total_timesteps  | 110521   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00403  |\n",
            "|    n_updates        | 27380    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 555      |\n",
            "|    ep_rew_mean      | -264     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 364      |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 203      |\n",
            "|    total_timesteps  | 113929   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0103   |\n",
            "|    n_updates        | 28232    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 568      |\n",
            "|    ep_rew_mean      | -270     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 368      |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 208      |\n",
            "|    total_timesteps  | 116509   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.335    |\n",
            "|    n_updates        | 28877    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 582      |\n",
            "|    ep_rew_mean      | -277     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 372      |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 214      |\n",
            "|    total_timesteps  | 119133   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0118   |\n",
            "|    n_updates        | 29533    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 603      |\n",
            "|    ep_rew_mean      | -288     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 376      |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 221      |\n",
            "|    total_timesteps  | 122613   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0164   |\n",
            "|    n_updates        | 30403    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 623      |\n",
            "|    ep_rew_mean      | -297     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 227      |\n",
            "|    total_timesteps  | 125577   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0062   |\n",
            "|    n_updates        | 31144    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 635      |\n",
            "|    ep_rew_mean      | -303     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 384      |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 232      |\n",
            "|    total_timesteps  | 128117   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0146   |\n",
            "|    n_updates        | 31779    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 656      |\n",
            "|    ep_rew_mean      | -313     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 388      |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 239      |\n",
            "|    total_timesteps  | 131673   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0047   |\n",
            "|    n_updates        | 32668    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 671      |\n",
            "|    ep_rew_mean      | -321     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 392      |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 244      |\n",
            "|    total_timesteps  | 134157   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00405  |\n",
            "|    n_updates        | 33289    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 690      |\n",
            "|    ep_rew_mean      | -330     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 396      |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 252      |\n",
            "|    total_timesteps  | 137766   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00479  |\n",
            "|    n_updates        | 34191    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 701      |\n",
            "|    ep_rew_mean      | -335     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 257      |\n",
            "|    total_timesteps  | 140585   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.36     |\n",
            "|    n_updates        | 34896    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 709      |\n",
            "|    ep_rew_mean      | -339     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 404      |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 263      |\n",
            "|    total_timesteps  | 143113   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0112   |\n",
            "|    n_updates        | 35528    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | -347     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 408      |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 270      |\n",
            "|    total_timesteps  | 146510   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.01     |\n",
            "|    n_updates        | 36377    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 715      |\n",
            "|    ep_rew_mean      | -341     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 412      |\n",
            "|    fps              | 540      |\n",
            "|    time_elapsed     | 274      |\n",
            "|    total_timesteps  | 148577   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00792  |\n",
            "|    n_updates        | 36894    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 705      |\n",
            "|    ep_rew_mean      | -337     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 416      |\n",
            "|    fps              | 540      |\n",
            "|    time_elapsed     | 280      |\n",
            "|    total_timesteps  | 151629   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00536  |\n",
            "|    n_updates        | 37657    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 710      |\n",
            "|    ep_rew_mean      | -339     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 536      |\n",
            "|    time_elapsed     | 288      |\n",
            "|    total_timesteps  | 154965   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0116   |\n",
            "|    n_updates        | 38491    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 720      |\n",
            "|    ep_rew_mean      | -344     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 424      |\n",
            "|    fps              | 536      |\n",
            "|    time_elapsed     | 295      |\n",
            "|    total_timesteps  | 158245   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0184   |\n",
            "|    n_updates        | 39311    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 728      |\n",
            "|    ep_rew_mean      | -347     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 428      |\n",
            "|    fps              | 534      |\n",
            "|    time_elapsed     | 301      |\n",
            "|    total_timesteps  | 161045   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0076   |\n",
            "|    n_updates        | 40011    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 727      |\n",
            "|    ep_rew_mean      | -347     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 432      |\n",
            "|    fps              | 534      |\n",
            "|    time_elapsed     | 306      |\n",
            "|    total_timesteps  | 163889   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.373    |\n",
            "|    n_updates        | 40722    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 721      |\n",
            "|    ep_rew_mean      | -343     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 436      |\n",
            "|    fps              | 532      |\n",
            "|    time_elapsed     | 312      |\n",
            "|    total_timesteps  | 166673   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0102   |\n",
            "|    n_updates        | 41418    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 706      |\n",
            "|    ep_rew_mean      | -336     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 440      |\n",
            "|    fps              | 532      |\n",
            "|    time_elapsed     | 316      |\n",
            "|    total_timesteps  | 168673   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00802  |\n",
            "|    n_updates        | 41918    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 707      |\n",
            "|    ep_rew_mean      | -336     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 444      |\n",
            "|    fps              | 532      |\n",
            "|    time_elapsed     | 320      |\n",
            "|    total_timesteps  | 170709   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00271  |\n",
            "|    n_updates        | 42427    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 703      |\n",
            "|    ep_rew_mean      | -335     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 448      |\n",
            "|    fps              | 530      |\n",
            "|    time_elapsed     | 326      |\n",
            "|    total_timesteps  | 173301   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.372    |\n",
            "|    n_updates        | 43075    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 702      |\n",
            "|    ep_rew_mean      | -334     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 452      |\n",
            "|    fps              | 530      |\n",
            "|    time_elapsed     | 331      |\n",
            "|    total_timesteps  | 175797   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.369    |\n",
            "|    n_updates        | 43699    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 700      |\n",
            "|    ep_rew_mean      | -333     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 456      |\n",
            "|    fps              | 530      |\n",
            "|    time_elapsed     | 335      |\n",
            "|    total_timesteps  | 177953   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0157   |\n",
            "|    n_updates        | 44238    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 688      |\n",
            "|    ep_rew_mean      | -327     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 460      |\n",
            "|    fps              | 529      |\n",
            "|    time_elapsed     | 338      |\n",
            "|    total_timesteps  | 179313   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0114   |\n",
            "|    n_updates        | 44578    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 670      |\n",
            "|    ep_rew_mean      | -318     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 464      |\n",
            "|    fps              | 528      |\n",
            "|    time_elapsed     | 342      |\n",
            "|    total_timesteps  | 180945   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00687  |\n",
            "|    n_updates        | 44986    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 661      |\n",
            "|    ep_rew_mean      | -314     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 468      |\n",
            "|    fps              | 528      |\n",
            "|    time_elapsed     | 345      |\n",
            "|    total_timesteps  | 182641   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00581  |\n",
            "|    n_updates        | 45410    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 654      |\n",
            "|    ep_rew_mean      | -310     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 472      |\n",
            "|    fps              | 527      |\n",
            "|    time_elapsed     | 349      |\n",
            "|    total_timesteps  | 184557   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 45889    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 639      |\n",
            "|    ep_rew_mean      | -303     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 476      |\n",
            "|    fps              | 527      |\n",
            "|    time_elapsed     | 353      |\n",
            "|    total_timesteps  | 186477   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.013    |\n",
            "|    n_updates        | 46369    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 619      |\n",
            "|    ep_rew_mean      | -293     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 480      |\n",
            "|    fps              | 527      |\n",
            "|    time_elapsed     | 355      |\n",
            "|    total_timesteps  | 187445   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00866  |\n",
            "|    n_updates        | 46611    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 609      |\n",
            "|    ep_rew_mean      | -288     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 484      |\n",
            "|    fps              | 527      |\n",
            "|    time_elapsed     | 358      |\n",
            "|    total_timesteps  | 189009   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00851  |\n",
            "|    n_updates        | 47002    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 586      |\n",
            "|    ep_rew_mean      | -277     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 488      |\n",
            "|    fps              | 526      |\n",
            "|    time_elapsed     | 361      |\n",
            "|    total_timesteps  | 190289   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00836  |\n",
            "|    n_updates        | 47322    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 577      |\n",
            "|    ep_rew_mean      | -272     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 492      |\n",
            "|    fps              | 525      |\n",
            "|    time_elapsed     | 364      |\n",
            "|    total_timesteps  | 191826   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00857  |\n",
            "|    n_updates        | 47706    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 559      |\n",
            "|    ep_rew_mean      | -263     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 496      |\n",
            "|    fps              | 525      |\n",
            "|    time_elapsed     | 368      |\n",
            "|    total_timesteps  | 193625   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0064   |\n",
            "|    n_updates        | 48156    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 545      |\n",
            "|    ep_rew_mean      | -256     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 500      |\n",
            "|    fps              | 525      |\n",
            "|    time_elapsed     | 371      |\n",
            "|    total_timesteps  | 195097   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00832  |\n",
            "|    n_updates        | 48524    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 536      |\n",
            "|    ep_rew_mean      | -252     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 504      |\n",
            "|    fps              | 525      |\n",
            "|    time_elapsed     | 374      |\n",
            "|    total_timesteps  | 196689   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0157   |\n",
            "|    n_updates        | 48922    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 509      |\n",
            "|    ep_rew_mean      | -239     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 508      |\n",
            "|    fps              | 524      |\n",
            "|    time_elapsed     | 376      |\n",
            "|    total_timesteps  | 197405   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00677  |\n",
            "|    n_updates        | 49101    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | -234     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 512      |\n",
            "|    fps              | 524      |\n",
            "|    time_elapsed     | 378      |\n",
            "|    total_timesteps  | 198602   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.384    |\n",
            "|    n_updates        | 49400    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 481      |\n",
            "|    ep_rew_mean      | -225     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 516      |\n",
            "|    fps              | 524      |\n",
            "|    time_elapsed     | 381      |\n",
            "|    total_timesteps  | 199761   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00756  |\n",
            "|    n_updates        | 49690    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 461      |\n",
            "|    ep_rew_mean      | -215     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 520      |\n",
            "|    fps              | 524      |\n",
            "|    time_elapsed     | 383      |\n",
            "|    total_timesteps  | 201085   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00979  |\n",
            "|    n_updates        | 50021    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -205     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 524      |\n",
            "|    fps              | 524      |\n",
            "|    time_elapsed     | 385      |\n",
            "|    total_timesteps  | 202305   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0164   |\n",
            "|    n_updates        | 50326    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 425      |\n",
            "|    ep_rew_mean      | -197     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 528      |\n",
            "|    fps              | 523      |\n",
            "|    time_elapsed     | 389      |\n",
            "|    total_timesteps  | 203526   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0105   |\n",
            "|    n_updates        | 50631    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 532      |\n",
            "|    fps              | 522      |\n",
            "|    time_elapsed     | 391      |\n",
            "|    total_timesteps  | 204785   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00997  |\n",
            "|    n_updates        | 50946    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 536      |\n",
            "|    fps              | 522      |\n",
            "|    time_elapsed     | 394      |\n",
            "|    total_timesteps  | 206121   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0069   |\n",
            "|    n_updates        | 51280    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 540      |\n",
            "|    fps              | 522      |\n",
            "|    time_elapsed     | 396      |\n",
            "|    total_timesteps  | 207053   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0167   |\n",
            "|    n_updates        | 51513    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 544      |\n",
            "|    fps              | 522      |\n",
            "|    time_elapsed     | 400      |\n",
            "|    total_timesteps  | 209243   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00814  |\n",
            "|    n_updates        | 52060    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | -171     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 548      |\n",
            "|    fps              | 521      |\n",
            "|    time_elapsed     | 403      |\n",
            "|    total_timesteps  | 210425   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00512  |\n",
            "|    n_updates        | 52356    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 552      |\n",
            "|    fps              | 521      |\n",
            "|    time_elapsed     | 406      |\n",
            "|    total_timesteps  | 211913   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00563  |\n",
            "|    n_updates        | 52728    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 352      |\n",
            "|    ep_rew_mean      | -161     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 556      |\n",
            "|    fps              | 521      |\n",
            "|    time_elapsed     | 408      |\n",
            "|    total_timesteps  | 213121   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00777  |\n",
            "|    n_updates        | 53030    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 352      |\n",
            "|    ep_rew_mean      | -162     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 560      |\n",
            "|    fps              | 521      |\n",
            "|    time_elapsed     | 411      |\n",
            "|    total_timesteps  | 214485   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00408  |\n",
            "|    n_updates        | 53371    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 351      |\n",
            "|    ep_rew_mean      | -161     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 564      |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 415      |\n",
            "|    total_timesteps  | 216069   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00811  |\n",
            "|    n_updates        | 53767    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 350      |\n",
            "|    ep_rew_mean      | -161     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 568      |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 418      |\n",
            "|    total_timesteps  | 217689   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00957  |\n",
            "|    n_updates        | 54172    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 348      |\n",
            "|    ep_rew_mean      | -160     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 572      |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 421      |\n",
            "|    total_timesteps  | 219369   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00252  |\n",
            "|    n_updates        | 54592    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 341      |\n",
            "|    ep_rew_mean      | -156     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 576      |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 423      |\n",
            "|    total_timesteps  | 220529   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00906  |\n",
            "|    n_updates        | 54882    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 339      |\n",
            "|    ep_rew_mean      | -155     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 580      |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 425      |\n",
            "|    total_timesteps  | 221377   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00432  |\n",
            "|    n_updates        | 55094    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | -154     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 584      |\n",
            "|    fps              | 519      |\n",
            "|    time_elapsed     | 428      |\n",
            "|    total_timesteps  | 222657   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0124   |\n",
            "|    n_updates        | 55414    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 342      |\n",
            "|    ep_rew_mean      | -157     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 588      |\n",
            "|    fps              | 519      |\n",
            "|    time_elapsed     | 432      |\n",
            "|    total_timesteps  | 224525   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.4      |\n",
            "|    n_updates        | 55881    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 334      |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 592      |\n",
            "|    fps              | 519      |\n",
            "|    time_elapsed     | 433      |\n",
            "|    total_timesteps  | 225237   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.395    |\n",
            "|    n_updates        | 56059    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 329      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 596      |\n",
            "|    fps              | 519      |\n",
            "|    time_elapsed     | 436      |\n",
            "|    total_timesteps  | 226530   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0107   |\n",
            "|    n_updates        | 56382    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 328      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 600      |\n",
            "|    fps              | 518      |\n",
            "|    time_elapsed     | 439      |\n",
            "|    total_timesteps  | 227925   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00535  |\n",
            "|    n_updates        | 56731    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 325      |\n",
            "|    ep_rew_mean      | -148     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 604      |\n",
            "|    fps              | 518      |\n",
            "|    time_elapsed     | 442      |\n",
            "|    total_timesteps  | 229201   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.394    |\n",
            "|    n_updates        | 57050    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 330      |\n",
            "|    ep_rew_mean      | -151     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 608      |\n",
            "|    fps              | 518      |\n",
            "|    time_elapsed     | 444      |\n",
            "|    total_timesteps  | 230357   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0142   |\n",
            "|    n_updates        | 57339    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 330      |\n",
            "|    ep_rew_mean      | -151     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 612      |\n",
            "|    fps              | 518      |\n",
            "|    time_elapsed     | 446      |\n",
            "|    total_timesteps  | 231641   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00528  |\n",
            "|    n_updates        | 57660    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 333      |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 616      |\n",
            "|    fps              | 518      |\n",
            "|    time_elapsed     | 449      |\n",
            "|    total_timesteps  | 233077   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.398    |\n",
            "|    n_updates        | 58019    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 335      |\n",
            "|    ep_rew_mean      | -154     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 620      |\n",
            "|    fps              | 517      |\n",
            "|    time_elapsed     | 453      |\n",
            "|    total_timesteps  | 234601   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00557  |\n",
            "|    n_updates        | 58400    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 339      |\n",
            "|    ep_rew_mean      | -156     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 624      |\n",
            "|    fps              | 517      |\n",
            "|    time_elapsed     | 456      |\n",
            "|    total_timesteps  | 236249   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0081   |\n",
            "|    n_updates        | 58812    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 340      |\n",
            "|    ep_rew_mean      | -156     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 628      |\n",
            "|    fps              | 517      |\n",
            "|    time_elapsed     | 459      |\n",
            "|    total_timesteps  | 237545   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0099   |\n",
            "|    n_updates        | 59136    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 340      |\n",
            "|    ep_rew_mean      | -156     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 632      |\n",
            "|    fps              | 517      |\n",
            "|    time_elapsed     | 461      |\n",
            "|    total_timesteps  | 238797   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0068   |\n",
            "|    n_updates        | 59449    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 337      |\n",
            "|    ep_rew_mean      | -155     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 636      |\n",
            "|    fps              | 517      |\n",
            "|    time_elapsed     | 463      |\n",
            "|    total_timesteps  | 239849   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00833  |\n",
            "|    n_updates        | 59712    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 337      |\n",
            "|    ep_rew_mean      | -154     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 640      |\n",
            "|    fps              | 516      |\n",
            "|    time_elapsed     | 466      |\n",
            "|    total_timesteps  | 240769   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.396    |\n",
            "|    n_updates        | 59942    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 324      |\n",
            "|    ep_rew_mean      | -148     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 644      |\n",
            "|    fps              | 516      |\n",
            "|    time_elapsed     | 467      |\n",
            "|    total_timesteps  | 241618   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0128   |\n",
            "|    n_updates        | 60154    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 320      |\n",
            "|    ep_rew_mean      | -146     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 648      |\n",
            "|    fps              | 516      |\n",
            "|    time_elapsed     | 469      |\n",
            "|    total_timesteps  | 242441   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.786    |\n",
            "|    n_updates        | 60360    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 323      |\n",
            "|    ep_rew_mean      | -147     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 652      |\n",
            "|    fps              | 516      |\n",
            "|    time_elapsed     | 472      |\n",
            "|    total_timesteps  | 244197   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00686  |\n",
            "|    n_updates        | 60799    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 335      |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 656      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 478      |\n",
            "|    total_timesteps  | 246589   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0048   |\n",
            "|    n_updates        | 61397    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 329      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 660      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 479      |\n",
            "|    total_timesteps  | 247389   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.398    |\n",
            "|    n_updates        | 61597    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 324      |\n",
            "|    ep_rew_mean      | -147     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 664      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 481      |\n",
            "|    total_timesteps  | 248453   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.4      |\n",
            "|    n_updates        | 61863    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 320      |\n",
            "|    ep_rew_mean      | -145     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 668      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 484      |\n",
            "|    total_timesteps  | 249669   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00534  |\n",
            "|    n_updates        | 62167    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 311      |\n",
            "|    ep_rew_mean      | -141     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 672      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 485      |\n",
            "|    total_timesteps  | 250481   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00507  |\n",
            "|    n_updates        | 62370    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 313      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 676      |\n",
            "|    fps              | 515      |\n",
            "|    time_elapsed     | 488      |\n",
            "|    total_timesteps  | 251867   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00704  |\n",
            "|    n_updates        | 62716    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 315      |\n",
            "|    ep_rew_mean      | -143     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 680      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 491      |\n",
            "|    total_timesteps  | 252897   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00801  |\n",
            "|    n_updates        | 62974    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 316      |\n",
            "|    ep_rew_mean      | -143     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 684      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 493      |\n",
            "|    total_timesteps  | 254213   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00415  |\n",
            "|    n_updates        | 63303    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 310      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 688      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 496      |\n",
            "|    total_timesteps  | 255497   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0132   |\n",
            "|    n_updates        | 63624    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 317      |\n",
            "|    ep_rew_mean      | -144     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 692      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 499      |\n",
            "|    total_timesteps  | 256961   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00409  |\n",
            "|    n_updates        | 63990    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 314      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 696      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 500      |\n",
            "|    total_timesteps  | 257893   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00505  |\n",
            "|    n_updates        | 64223    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 306      |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 700      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 502      |\n",
            "|    total_timesteps  | 258573   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0147   |\n",
            "|    n_updates        | 64393    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 304      |\n",
            "|    ep_rew_mean      | -137     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 704      |\n",
            "|    fps              | 514      |\n",
            "|    time_elapsed     | 505      |\n",
            "|    total_timesteps  | 259577   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0067   |\n",
            "|    n_updates        | 64644    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 308      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 708      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 508      |\n",
            "|    total_timesteps  | 261161   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.404    |\n",
            "|    n_updates        | 65040    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 309      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 712      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 510      |\n",
            "|    total_timesteps  | 262561   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00783  |\n",
            "|    n_updates        | 65390    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 306      |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 716      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 512      |\n",
            "|    total_timesteps  | 263641   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.409    |\n",
            "|    n_updates        | 65660    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 299      |\n",
            "|    ep_rew_mean      | -135     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 514      |\n",
            "|    total_timesteps  | 264533   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.003    |\n",
            "|    n_updates        | 65883    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 294      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 724      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 517      |\n",
            "|    total_timesteps  | 265645   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00731  |\n",
            "|    n_updates        | 66161    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 294      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 728      |\n",
            "|    fps              | 513      |\n",
            "|    time_elapsed     | 520      |\n",
            "|    total_timesteps  | 266969   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.412    |\n",
            "|    n_updates        | 66492    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 295      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 732      |\n",
            "|    fps              | 512      |\n",
            "|    time_elapsed     | 523      |\n",
            "|    total_timesteps  | 268345   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0092   |\n",
            "|    n_updates        | 66836    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 295      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 736      |\n",
            "|    fps              | 512      |\n",
            "|    time_elapsed     | 525      |\n",
            "|    total_timesteps  | 269373   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00743  |\n",
            "|    n_updates        | 67093    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 298      |\n",
            "|    ep_rew_mean      | -135     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 740      |\n",
            "|    fps              | 512      |\n",
            "|    time_elapsed     | 528      |\n",
            "|    total_timesteps  | 270609   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.411    |\n",
            "|    n_updates        | 67402    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 305      |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 744      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 531      |\n",
            "|    total_timesteps  | 272133   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00783  |\n",
            "|    n_updates        | 67783    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 307      |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 748      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 533      |\n",
            "|    total_timesteps  | 273190   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.413    |\n",
            "|    n_updates        | 68047    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 299      |\n",
            "|    ep_rew_mean      | -135     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 752      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 535      |\n",
            "|    total_timesteps  | 274133   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00607  |\n",
            "|    n_updates        | 68283    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 756      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 538      |\n",
            "|    total_timesteps  | 275577   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00548  |\n",
            "|    n_updates        | 68644    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 289      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 760      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 539      |\n",
            "|    total_timesteps  | 276317   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.413    |\n",
            "|    n_updates        | 68829    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 292      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 764      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 543      |\n",
            "|    total_timesteps  | 277649   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.422    |\n",
            "|    n_updates        | 69162    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 768      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 545      |\n",
            "|    total_timesteps  | 278713   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00568  |\n",
            "|    n_updates        | 69428    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 294      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 772      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 547      |\n",
            "|    total_timesteps  | 279877   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00623  |\n",
            "|    n_updates        | 69719    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 289      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 776      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 549      |\n",
            "|    total_timesteps  | 280805   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.812    |\n",
            "|    n_updates        | 69951    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 293      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 780      |\n",
            "|    fps              | 511      |\n",
            "|    time_elapsed     | 551      |\n",
            "|    total_timesteps  | 282157   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00241  |\n",
            "|    n_updates        | 70289    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 294      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 784      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 555      |\n",
            "|    total_timesteps  | 283661   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00491  |\n",
            "|    n_updates        | 70665    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 293      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 788      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 557      |\n",
            "|    total_timesteps  | 284785   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00478  |\n",
            "|    n_updates        | 70946    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 289      |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 792      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 559      |\n",
            "|    total_timesteps  | 285817   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 71204    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 292      |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 796      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 562      |\n",
            "|    total_timesteps  | 287110   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00645  |\n",
            "|    n_updates        | 71527    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 302      |\n",
            "|    ep_rew_mean      | -137     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 800      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 565      |\n",
            "|    total_timesteps  | 288785   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.428    |\n",
            "|    n_updates        | 71946    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 303      |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 804      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 568      |\n",
            "|    total_timesteps  | 289889   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00373  |\n",
            "|    n_updates        | 72222    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 301      |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 808      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 570      |\n",
            "|    total_timesteps  | 291241   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0133   |\n",
            "|    n_updates        | 72560    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 295      |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 812      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 572      |\n",
            "|    total_timesteps  | 292041   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0101   |\n",
            "|    n_updates        | 72760    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 816      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 573      |\n",
            "|    total_timesteps  | 292609   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0102   |\n",
            "|    n_updates        | 72902    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 820      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 575      |\n",
            "|    total_timesteps  | 293505   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.42     |\n",
            "|    n_updates        | 73126    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 288      |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 824      |\n",
            "|    fps              | 510      |\n",
            "|    time_elapsed     | 577      |\n",
            "|    total_timesteps  | 294461   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0124   |\n",
            "|    n_updates        | 73365    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 281      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 828      |\n",
            "|    fps              | 509      |\n",
            "|    time_elapsed     | 578      |\n",
            "|    total_timesteps  | 295105   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00459  |\n",
            "|    n_updates        | 73526    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 280      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 832      |\n",
            "|    fps              | 509      |\n",
            "|    time_elapsed     | 581      |\n",
            "|    total_timesteps  | 296381   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0113   |\n",
            "|    n_updates        | 73845    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 278      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 836      |\n",
            "|    fps              | 509      |\n",
            "|    time_elapsed     | 583      |\n",
            "|    total_timesteps  | 297193   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00764  |\n",
            "|    n_updates        | 74048    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 274      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 840      |\n",
            "|    fps              | 509      |\n",
            "|    time_elapsed     | 584      |\n",
            "|    total_timesteps  | 298001   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0146   |\n",
            "|    n_updates        | 74250    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 269      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 844      |\n",
            "|    fps              | 509      |\n",
            "|    time_elapsed     | 586      |\n",
            "|    total_timesteps  | 299077   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00369  |\n",
            "|    n_updates        | 74519    |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x782f41cc63b0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO"
      ],
      "metadata": {
        "id": "DxHnJuk2e3ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "\n",
        "# Define hyperparameters for PPO\n",
        "learning_rate = 1e-4       # Learning rate for PPO\n",
        "n_steps = 2048             # Number of steps to run for each environment per update\n",
        "batch_size = 64            # Batch size for each update\n",
        "n_epochs = 10              # Number of times to train on each batch\n",
        "gamma = 0.99               # Discount factor\n",
        "gae_lambda = 0.95          # GAE lambda, for variance reduction in advantage estimation\n",
        "clip_range = 0.2           # Clip range for PPO, helps with stable training\n",
        "\n",
        "# Instantiate PPO with custom hyperparameters\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",           # Policy type, can try \"CnnPolicy\" for image-based inputs\n",
        "    vec_env,\n",
        "    learning_rate=learning_rate,\n",
        "    n_steps=n_steps,\n",
        "    batch_size=batch_size,\n",
        "    n_epochs=n_epochs,\n",
        "    gamma=gamma,\n",
        "    gae_lambda=gae_lambda,\n",
        "    clip_range=clip_range,\n",
        "    verbose=1               # Verbose output\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "094qjCQoTEvo",
        "outputId": "4d8a779d-185c-49fc-a0ee-12e43de4019d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.63e+03 |\n",
            "|    ep_rew_mean     | -9.6e+05 |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 207       |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 25.478064 |\n",
            "|    clip_fraction        | 0.66      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.98     |\n",
            "|    explained_variance   | -6.08e-06 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.85e+05  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | 0.26      |\n",
            "|    value_loss           | 4.35e+07  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 203       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.73e-09 |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.01e+04  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -0.000606 |\n",
            "|    value_loss           | 5.41e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 207       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 39        |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.33e-11 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.69e+04  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 3.3e-06   |\n",
            "|    value_loss           | 3.77e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 206       |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 49        |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.02e-13 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.95e+03  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | 0.000224  |\n",
            "|    value_loss           | 2.12e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 202       |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 60        |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.08e-14 |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.33e+04  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -1.41e-05 |\n",
            "|    value_loss           | 1.14e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 201       |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 71        |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.85e-16 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 597       |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | 0.00179   |\n",
            "|    value_loss           | 8.88e+04  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 202       |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 81        |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.38e-17 |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.1e+03   |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | 2.4e-06   |\n",
            "|    value_loss           | 4.87e+04  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 201       |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 91        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.15e-17 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.9e+03   |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | 0.000257  |\n",
            "|    value_loss           | 4.15e+04  |\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "3UzT_BZle8Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "vec_env2 = make_vec_env(MinesweeperEnvironment, n_envs=1, env_kwargs=dict(end_on_bomb=True))\n",
        "obs = vec_env2.reset()\n",
        "n_steps = 100\n",
        "total_substeps = 0\n",
        "for step in range(n_steps):\n",
        "    # action, _ = model.predict(obs, deterministic=False)\n",
        "\n",
        "    # Convert the observation to a tensor and ensure it's on the same device as the model\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(model.device)\n",
        "\n",
        "    # Get Q-values directly from the model's q_net\n",
        "    with torch.no_grad():\n",
        "        q_values = model.q_net(obs_tensor).cpu().numpy()\n",
        "\n",
        "    # Flatten the Q-values and get indices in descending order\n",
        "    ranked_actions = q_values[0].argsort()[::-1]\n",
        "\n",
        "    print(f\"found {len(ranked_actions)} ranked actions\")\n",
        "\n",
        "    for k, action_k in enumerate(ranked_actions):\n",
        "      total_substeps += 1\n",
        "      print(f\"trying action {k}\")\n",
        "      x = int(action_k) // 9\n",
        "      y = int(action_k) % 9\n",
        "      print(\"Action: \", (x, y), action_k)\n",
        "      obs, reward, done, info = vec_env2.step([action_k])\n",
        "      effect = info[0][\"effect\"]\n",
        "      print(\"reward=\", reward, \"done=\", done, \"effect=\", effect)\n",
        "      vec_env2.render()\n",
        "\n",
        "      if reward > 0:\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print(\"Won!\" if effect == GAME_WIN else \"Lost :(\")\n",
        "        print(\"total_steps:\", step)\n",
        "        print(\"total_substeps:\", total_substeps)\n",
        "        break"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrxjJU58AYSP",
        "outputId": "6fcffdb5-f2e8-4c7b-a053-5420b2d9a2a6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (5, 5) 50\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 1\n",
            "Action:  (3, 3) 30\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 2\n",
            "Action:  (4, 3) 39\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 3\n",
            "Action:  (5, 4) 49\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 4\n",
            "Action:  (5, 6) 51\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 5\n",
            "Action:  (0, 2) 2\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 6\n",
            "Action:  (2, 2) 20\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 7\n",
            "Action:  (0, 0) 0\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 8\n",
            "Action:  (6, 6) 60\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 9\n",
            "Action:  (1, 2) 11\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 10\n",
            "Action:  (1, 3) 12\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 11\n",
            "Action:  (3, 2) 29\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 12\n",
            "Action:  (5, 8) 53\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 13\n",
            "Action:  (6, 7) 61\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 14\n",
            "Action:  (0, 3) 3\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 26 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "trying action 15\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * 1 * * * * * * * \n",
            "trying action 16\n",
            "Action:  (1, 0) 9\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * 1 * * * * * * * \n",
            "trying action 17\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * 1 * * * * * * * \n",
            "trying action 18\n",
            "Action:  (7, 1) 64\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 24 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * * * 1 1 1 1 \n",
            "8 * 1 * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 3) 66\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 23 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 22 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 6) 24\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 21 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 21 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 1\n",
            "Action:  (4, 0) 36\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 2\n",
            "Action:  (3, 6) 33\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 2\n",
            "Action:  (6, 0) 54\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 2\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 3\n",
            "Action:  (1, 6) 15\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * * * * 1 \n",
            "trying action 2\n",
            "Action:  (8, 5) 77\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 2\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 3\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 4\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 5\n",
            "Action:  (4, 8) 44\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 6\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 7\n",
            "Action:  (5, 0) 45\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 2\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 3\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 4\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 5\n",
            "Action:  (6, 2) 56\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 6\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 7\n",
            "Action:  (4, 8) 44\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 8\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 9\n",
            "Action:  (2, 4) 22\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 10\n",
            "Action:  (1, 6) 15\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * * * 1 * * 1 \n",
            "trying action 11\n",
            "Action:  (8, 3) 75\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 1\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 2\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 3\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 4\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 5\n",
            "Action:  (6, 2) 56\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 6\n",
            "Action:  (4, 8) 44\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 7\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 8\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 9\n",
            "Action:  (2, 4) 22\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 * * 1 \n",
            "trying action 10\n",
            "Action:  (8, 6) 78\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 2\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 3\n",
            "Action:  (0, 8) 8\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 4\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 5\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 6\n",
            "Action:  (2, 4) 22\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 7\n",
            "Action:  (1, 0) 9\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 8\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 9\n",
            "Action:  (4, 8) 44\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 10\n",
            "Action:  (1, 6) 15\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 11\n",
            "Action:  (4, 1) 37\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 12\n",
            "Action:  (6, 2) 56\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 13\n",
            "Action:  (0, 7) 7\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 14\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 15\n",
            "Action:  (3, 0) 27\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 6 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 6 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 1\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 6 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 2\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 6 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * 1 * 1 * 1 1 * 1 \n",
            "trying action 3\n",
            "Action:  (8, 2) 74\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 1\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 2\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 3\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 4\n",
            "Action:  (6, 2) 56\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 5\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 6\n",
            "Action:  (3, 8) 35\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 1\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 2\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 3\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 4\n",
            "Action:  (1, 5) 14\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * 1 1 * 1 \n",
            "trying action 5\n",
            "Action:  (8, 0) 72\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 1\n",
            "Action:  (8, 1) 73\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 2\n",
            "Action:  (7, 3) 66\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 3\n",
            "Action:  (6, 0) 54\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 4\n",
            "Action:  (7, 7) 70\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 5\n",
            "Action:  (4, 6) 42\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 6\n",
            "Action:  (2, 6) 24\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 7\n",
            "Action:  (7, 1) 64\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 8\n",
            "Action:  (1, 6) 15\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 9\n",
            "Action:  (4, 8) 44\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 10\n",
            "Action:  (1, 1) 10\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 11\n",
            "Action:  (0, 8) 8\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 12\n",
            "Action:  (6, 2) 56\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 13\n",
            "Action:  (0, 0) 0\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 14\n",
            "Action:  (1, 5) 14\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 15\n",
            "Action:  (8, 5) 77\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 16\n",
            "Action:  (0, 2) 2\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 17\n",
            "Action:  (1, 0) 9\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 18\n",
            "Action:  (5, 8) 53\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 19\n",
            "Action:  (4, 1) 37\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 20\n",
            "Action:  (4, 7) 43\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * 1 1 * 1 \n",
            "trying action 21\n",
            "Action:  (8, 4) 76\n",
            "reward= [1.] done= [ True] effect= game-win\n",
            "Current Board: 73 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "Won!\n",
            "total_steps: 16\n",
            "total_substeps: 123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old RL Zoo code"
      ],
      "metadata": {
        "id": "BM0Mu-fpw4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from rl_zoo3.train import train\n",
        "# from gym.envs.registration import register\n",
        "\n",
        "# register(\n",
        "#     id='Minesweeper-v1',\n",
        "#     entry_point='msenv:MinesweeperEnvironment',  # Update '__main__' to the module name if this is not in your main script\n",
        "#     max_episode_steps=100,  # Adjust based on expected game length\n",
        "# )\n",
        "\n",
        "# import gym\n",
        "# print([k for k in gym.envs.registry.keys() if \"Minesweeper\" in k])\n",
        "\n",
        "# !python -m rl_zoo3.train --algo dqn --env Minesweeper-v1 -f logs/ -c dqn.yml\n"
      ],
      "metadata": {
        "id": "DMBhu2n2sSky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}