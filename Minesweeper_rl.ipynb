{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pj0620/google-colab-notebooks/blob/main/Minesweeper_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from scipy.signal import convolve2d\n",
        "%pip install stable-baselines3[extra]\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "\n",
        "!apt-get install swig cmake ffmpeg\n",
        "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LZv1Xe1yddn4",
        "outputId": "5024f777-08ec-4643-b852-0e2cc2472854"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.3)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (10.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=bc6f82483acc7a96f27a7b4a704646cd45cb2aad8331efe69ac88952e7a5d448\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.3.0\n",
            "    Uninstalling Shimmy-1.3.0:\n",
            "      Successfully uninstalled Shimmy-1.3.0\n",
            "Successfully installed shimmy-0.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "shimmy"
                ]
              },
              "id": "741803ed97434480937073b297d521e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.6)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.8.30)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: autorom\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.6.1\n",
            "    Uninstalling AutoROM-0.6.1:\n",
            "      Successfully uninstalled AutoROM-0.6.1\n",
            "Successfully installed autorom-0.4.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (3,191 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting git+https://github.com/DLR-RM/rl-baselines3-zoo\n",
            "  Cloning https://github.com/DLR-RM/rl-baselines3-zoo to /tmp/pip-req-build-s159_45k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/DLR-RM/rl-baselines3-zoo /tmp/pip-req-build-s159_45k\n",
            "  Resolved https://github.com/DLR-RM/rl-baselines3-zoo to commit b1288edcf5c4902cebbdbc80b6bc65fb9baa0ccc\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sb3-contrib<3.0,>=2.4.0a10 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading sb3_contrib-2.4.0a10-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: gymnasium~=0.29.1 in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (0.29.1)\n",
            "Collecting huggingface-sb3<4.0,>=3.0 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (13.9.3)\n",
            "Collecting optuna>=3.0 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from rl_zoo3==2.4.0a10) (6.0.2)\n",
            "Collecting pytablewriter~=1.2 (from rl_zoo3==2.4.0a10)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium~=0.29.1->rl_zoo3==2.4.0a10) (0.0.4)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (0.24.7)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (1.1.3)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.0->rl_zoo3==2.4.0a10) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.0->rl_zoo3==2.4.0a10) (2.0.36)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter~=1.2->rl_zoo3==2.4.0a10) (75.1.0)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting stable-baselines3<3.0,>=2.4.0a6 (from sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10)\n",
            "  Downloading stable_baselines3-2.4.0a10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->rl_zoo3==2.4.0a10) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->rl_zoo3==2.4.0a10) (2.18.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.0->rl_zoo3==2.4.0a10)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2.32.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->rl_zoo3==2.4.0a10) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (5.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.0->rl_zoo3==2.4.0a10) (3.1.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.0->typepy[datetime]<2,>=1.3.2->pytablewriter~=1.2->rl_zoo3==2.4.0a10) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna>=3.0->rl_zoo3==2.4.0a10) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (3.2.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.4.0a6->sb3-contrib<3.0,>=2.4.0a10->rl_zoo3==2.4.0a10) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3<4.0,>=3.0->rl_zoo3==2.4.0a10) (2024.8.30)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sb3_contrib-2.4.0a10-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading stable_baselines3-2.4.0a10-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.5/183.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rl_zoo3\n",
            "  Building wheel for rl_zoo3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rl_zoo3: filename=rl_zoo3-2.4.0a10-py3-none-any.whl size=76997 sha256=a07658f292a12d5f2f9c09a5a79d7a4b106bdf6eba3cf06602b90e11051a3faa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jw4vq99z/wheels/5f/9b/c0/8af7ab740f894e9d66c7707eb4aa58087c9a71dd262e7e6174\n",
            "Successfully built rl_zoo3\n",
            "Installing collected packages: tcolorpy, pathvalidate, mbstrdecoder, Mako, colorlog, typepy, alembic, stable-baselines3, optuna, huggingface-sb3, sb3-contrib, DataProperty, tabledata, pytablewriter, rl_zoo3\n",
            "  Attempting uninstall: stable-baselines3\n",
            "    Found existing installation: stable_baselines3 2.3.2\n",
            "    Uninstalling stable_baselines3-2.3.2:\n",
            "      Successfully uninstalled stable_baselines3-2.3.2\n",
            "Successfully installed DataProperty-1.0.1 Mako-1.3.6 alembic-1.13.3 colorlog-6.9.0 huggingface-sb3-3.0 mbstrdecoder-1.1.3 optuna-4.0.0 pathvalidate-3.2.1 pytablewriter-1.2.0 rl_zoo3-2.4.0a10 sb3-contrib-2.4.0a10 stable-baselines3-2.4.0a10 tabledata-1.3.3 tcolorpy-0.1.6 typepy-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter configurations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BjNKEnmJ44KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a file named `dql.yml` with the following contents\n",
        "\n",
        "```\n",
        "Minesweeper-v1:\n",
        "  frame_stack: 1\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "```\n"
      ],
      "metadata": {
        "id": "mtC5i_SBwgyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom gymnasium env"
      ],
      "metadata": {
        "id": "qnhdrAKdlZdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from skimage.transform import resize\n",
        "\n",
        "CLICK_BOMB = \"click-bomb\"\n",
        "GAME_WIN = \"game-win\"\n",
        "GAME_LOSE = \"game-lose\"\n",
        "CLICK_VISIBLE = \"click-visible\"\n",
        "CLICK_GUESS = \"click-guess\"\n",
        "CLICK_VALID = \"click-valid\"\n",
        "\n",
        "DEFAULT_REWARDS = {\n",
        "    CLICK_VISIBLE: -0.5,\n",
        "    CLICK_BOMB: -1.,\n",
        "    GAME_WIN: 1,\n",
        "    CLICK_VALID: 0.3,\n",
        "    CLICK_GUESS: -0.3\n",
        "}\n",
        "\n",
        "class MinesweeperEnvironment(gym.Env):\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "  def __init__(self, board_size=9, total_bombs=8, render_mode=\"console\",\n",
        "               end_on_bomb=False, randomize_on_reset=False,\n",
        "               end_on_visible_click=False, rewards=None,\n",
        "               use_dict_space=False):\n",
        "    super().__init__()\n",
        "    self.render_mode = render_mode\n",
        "    self.board_size = board_size\n",
        "    self.total_bombs = total_bombs\n",
        "    self.end_on_bomb = end_on_bomb\n",
        "    self.randomize_on_reset = randomize_on_reset\n",
        "    self.end_on_visible_click = end_on_visible_click\n",
        "    self.use_dict_space = use_dict_space\n",
        "\n",
        "    self.visible = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "    self.set_bombs_vals()\n",
        "    # self.set_values()\n",
        "\n",
        "    if self.use_dict_space:\n",
        "      self.observation_space = spaces.Dict(dict(\n",
        "        board=spaces.Box(\n",
        "          low=-1, high=1, shape=(self.board_size, self.board_size, 2)\n",
        "        ))\n",
        "      )\n",
        "    else:\n",
        "      self.observation_space = spaces.Box(\n",
        "          low=-1, high=1, shape=(self.board_size, self.board_size, 2)\n",
        "      )\n",
        "\n",
        "    # Define action space\n",
        "    self.action_space = spaces.Discrete(self.board_size**2)\n",
        "\n",
        "    self.last_action = -1\n",
        "\n",
        "    if rewards is None:\n",
        "      self.rewards = DEFAULT_REWARDS\n",
        "    else:\n",
        "      self.rewards = rewards\n",
        "\n",
        "  def set_bombs_vals(self):\n",
        "    self.bombs = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "    self.vals = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "    # Generate unique random positions for bombs\n",
        "    bomb_positions = np.random.choice(self.board_size * self.board_size, self.total_bombs, replace=False)\n",
        "\n",
        "    # Convert linear indices to row, column indices\n",
        "    rows, cols = np.unravel_index(bomb_positions, (self.board_size, self.board_size))\n",
        "    self.bombs[rows, cols] = 1\n",
        "\n",
        "    # Increment neighbors for each bomb position using array slicing\n",
        "    for r, c in zip(rows, cols):\n",
        "      # Use slicing to add 1 to all neighboring cells in `self.vals`\n",
        "      self.vals[max(0, r - 1):min(self.board_size, r + 2),\n",
        "                max(0, c - 1):min(self.board_size, c + 2)] += 1\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    # Reset the environment to an initial state\n",
        "    self.visible = np.zeros((self.board_size, self.board_size), dtype=np.uint8)\n",
        "\n",
        "    # same everytime\n",
        "    if self.randomize_on_reset:\n",
        "      self.set_bombs_vals()\n",
        "    self.last_action = -1\n",
        "\n",
        "    return self.get_state(), {}\n",
        "\n",
        "  def propogate(self, x: int, y: int):\n",
        "    # If the initial cell is a bomb or already visible, return immediately\n",
        "    if self.bombs[x][y] == 1 or self.visible[x][y] == 1:\n",
        "      return\n",
        "\n",
        "    # Initialize a stack for iterative propagation\n",
        "    stack = [(x, y)]\n",
        "\n",
        "    while stack:\n",
        "      cx, cy = stack.pop()\n",
        "\n",
        "      # Skip cells already visible\n",
        "      if self.visible[cx][cy] == 1:\n",
        "        continue\n",
        "\n",
        "      # Mark current cell as visible\n",
        "      self.visible[cx][cy] = 1\n",
        "\n",
        "      # Only continue to neighbors if this cell has no adjacent bombs\n",
        "      if self.vals[cx][cy] == 0:\n",
        "        # Add all valid neighbors to the stack\n",
        "        for nx in range(max(0, cx - 1), min(self.board_size - 1, cx + 1) + 1):\n",
        "          for ny in range(max(0, cy - 1), min(self.board_size - 1, cy + 1) + 1):\n",
        "            # Skip the cell itself\n",
        "            if (nx, ny) != (cx, cy) and self.visible[nx][ny] == 0:\n",
        "              stack.append((nx, ny))\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # Implement the logic for taking a step in the environment\n",
        "    x = action // self.board_size\n",
        "    y = action % self.board_size\n",
        "\n",
        "    start_visible_cells = np.sum(self.visible)\n",
        "\n",
        "    if self.visible[x][y] == 1:\n",
        "      start_teminated = bool((self.board_size**2 - start_visible_cells) == self.total_bombs)\n",
        "      if self.end_on_visible_click:\n",
        "        return self.get_state(), self.rewards[CLICK_VISIBLE], True, True, {\"effect\": CLICK_VISIBLE}\n",
        "      else:\n",
        "        return self.get_state(), self.rewards[CLICK_VISIBLE], start_teminated, False, {\"effect\": CLICK_VISIBLE}\n",
        "    elif self.bombs[x][y] == 1:\n",
        "      if self.end_on_bomb:\n",
        "        return self.get_state(), self.rewards[CLICK_BOMB], True, True, {\"effect\": CLICK_BOMB}\n",
        "      else:\n",
        "        return self.get_state(), self.rewards[CLICK_BOMB], False, False, {\"effect\": CLICK_BOMB}\n",
        "\n",
        "    # allow first click to be guess if no other cells available\n",
        "    if start_visible_cells < 2:\n",
        "      is_guess_click = False\n",
        "    else:\n",
        "      # Set boundaries for slicing without going out of bounds\n",
        "      x_min = max(0, x - 1)\n",
        "      x_max = min(self.board_size - 1, x + 1)\n",
        "      y_min = max(0, y - 1)\n",
        "      y_max = min(self.board_size - 1, y + 1)\n",
        "\n",
        "      # Get neighborhood slice\n",
        "      neighborhood = self.visible[x_min:x_max+1, y_min:y_max+1]\n",
        "\n",
        "      # Check for any visible cells, excluding the center cell\n",
        "      is_guess_click = np.any(neighborhood)\n",
        "\n",
        "    self.last_action = action\n",
        "\n",
        "    if self.vals[x][y] == 0:\n",
        "      self.propogate(x, y)\n",
        "\n",
        "    self.visible[x][y] = 1\n",
        "    end_visible_cells = np.sum(self.visible)\n",
        "    teminated = bool((self.board_size**2 - end_visible_cells) == self.total_bombs)\n",
        "\n",
        "    if teminated:\n",
        "      return self.get_state(), self.rewards[GAME_WIN], teminated, False, {\"effect\": GAME_WIN}\n",
        "    else:\n",
        "      reward = self.rewards[CLICK_GUESS] if is_guess_click else self.rewards[CLICK_VALID]\n",
        "      info = CLICK_GUESS if is_guess_click else CLICK_VALID\n",
        "      return self.get_state(), reward, teminated, False, {\"effect\": info}\n",
        "\n",
        "  def get_state(self):\n",
        "    visible_vals = self.visible * self.vals\n",
        "    visible_vals = (visible_vals.astype(np.float32) - 4) / 4\n",
        "    visible_scaled = 2 * (self.visible.astype(np.float32) - 0.5)\n",
        "    board = np.stack([visible_vals, visible_scaled], axis=2).astype(np.float32)\n",
        "\n",
        "    if self.use_dict_space:\n",
        "      return dict(board=board)\n",
        "    else:\n",
        "      return board\n",
        "\n",
        "  def render(self, mode=\"console\"):\n",
        "    if self.render_mode != \"console\":\n",
        "        raise NotImplementedError(\"Render mode not supported.\")\n",
        "\n",
        "    # Print the current visible board state\n",
        "    cells_left = int(self.board_size**2 - np.sum(self.visible))\n",
        "    print(f\"Current Board: {cells_left - self.total_bombs} Cells Left\")\n",
        "    print(\"# \" + \" \".join(str(i) for i in range(self.board_size)))\n",
        "    for i in range(self.board_size):\n",
        "        print(f\"{i} \", end=\"\")\n",
        "        row = \"\"\n",
        "        for j in range(self.board_size):\n",
        "            if self.visible[i][j] == 1:\n",
        "                # If the cell is visible, show its value (number of adjacent bombs)\n",
        "                if self.bombs[i][j] == 1:\n",
        "                  row += f\"B \"\n",
        "                else:\n",
        "                  row += f\"{self.vals[i][j]} \"\n",
        "            else:\n",
        "                # If the cell is hidden, show an asterisk\n",
        "                row += \"* \"\n",
        "        print(row)\n",
        "    # print(\"\\n\")\n",
        "\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "env = MinesweeperEnvironment()\n",
        "\n",
        "# Testing env\n",
        "check_env(env)\n",
        "print(\"starting game\")\n",
        "for round in range(1):\n",
        "  obs, info = env.reset()\n",
        "  for _ in range(10):\n",
        "      # Random action\n",
        "      action = env.action_space.sample()\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "      if terminated:\n",
        "          obs, info = env.reset()\n",
        "      env.render()\n",
        "      reward_pos = f\"{reward * (env.board_size ** 2)} / {env.board_size ** 2}\"\n",
        "      print(f\"action: {action} -> {(action // env.board_size, action % env.board_size)}\")\n",
        "      # print(f\"reward: {reward if reward <= 0 else reward_pos}\")\n",
        "      print(f\"reward: {reward}\")\n",
        "      print(\"\\n\")\n",
        "  env.reset()"
      ],
      "metadata": {
        "id": "k1b3Xo2biryW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30b455d-21f0-49b4-dcac-4f88376af776"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting game\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * 1 * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "action: 40 -> (4, 4)\n",
            "reward: 0.3\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 47 -> (5, 2)\n",
            "reward: 0.3\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 36 -> (4, 0)\n",
            "reward: -1.0\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 61 -> (6, 7)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 30 -> (3, 3)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 58 -> (6, 4)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 13 -> (1, 4)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 20 -> (2, 2)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 59 -> (6, 5)\n",
            "reward: -0.5\n",
            "\n",
            "\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 1 * * * * \n",
            "1 0 0 0 0 2 * * * * \n",
            "2 0 0 0 0 2 * * 3 1 \n",
            "3 1 1 0 0 2 * 3 1 0 \n",
            "4 * 1 0 0 1 * 1 0 0 \n",
            "5 1 1 0 0 1 1 1 0 0 \n",
            "6 0 0 0 0 0 0 0 0 0 \n",
            "7 1 1 1 0 0 0 0 0 0 \n",
            "8 * * 1 0 0 0 0 0 0 \n",
            "action: 33 -> (3, 6)\n",
            "reward: -0.5\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:54: UserWarning: It seems that your observation  is an image but its `dtype` is (float32) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:62: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/env_checker.py:75: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play minesweeper"
      ],
      "metadata": {
        "id": "grsj8j6_ddhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = MinesweeperEnvironment(end_on_bomb=True)\n",
        "\n",
        "terminated = False\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "while not terminated:\n",
        "  actions_str = input(\"enter row then column with a space(\\\"X Y\\\"):\").split(\" \")\n",
        "  actions_int = [int(x) for x in actions_str]\n",
        "  action = actions_int[0] + actions_int[1] * env.board_size\n",
        "  obs, reward, terminated, truncated, info = env.step(action)\n",
        "  print(f\"reward: {reward}\")\n",
        "  env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1NlklL2dctR",
        "outputId": "75d938e5-b622-4962-add8-b4f09d5c0808"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Board: 73 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 3\n",
            "reward: -0.3\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 4\n",
            "reward: 0.3\n",
            "Current Board: 71 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 1 * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):5 4\n",
            "reward: -0.3\n",
            "Current Board: 70 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 1 * * * * * * * * \n",
            "4 1 * * * * 1 * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):4 5\n",
            "reward: 0.3\n",
            "Current Board: 24 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * * * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 3\n",
            "reward: 0.3\n",
            "Current Board: 23 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 3\n",
            "reward: 0.3\n",
            "Current Board: 22 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 2\n",
            "reward: 0.3\n",
            "Current Board: 21 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 2\n",
            "reward: 0.3\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 2\n",
            "reward: 0.3\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):6 1\n",
            "reward: 0.3\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 0\n",
            "reward: 0.3\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):7 1\n",
            "reward: 0.3\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):8 0\n",
            "reward: 0.3\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 5\n",
            "reward: 0.3\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 6\n",
            "reward: 0.3\n",
            "Current Board: 12 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):1 6\n",
            "reward: 0.3\n",
            "Current Board: 11 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * * * * * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "enter row then column with a space(\"X Y\"):2 7\n",
            "reward: 0.3\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 * 1 0 1 * * * * * \n",
            "enter row then column with a space(\"X Y\"):0 8\n",
            "reward: 0.3\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 * * * * * \n",
            "enter row then column with a space(\"X Y\"):4 8\n",
            "reward: 0.3\n",
            "Current Board: 3 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 * * * * \n",
            "enter row then column with a space(\"X Y\"):5 8\n",
            "reward: 0.3\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "enter row then column with a space(\"X Y\"):6 8\n",
            "reward: 0.3\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * * \n",
            "enter row then column with a space(\"X Y\"):8 8\n",
            "reward: 1\n",
            "Current Board: 0 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 1 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * 1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2U3JfCJ-fh7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-random Minesweeper"
      ],
      "metadata": {
        "id": "rijk1fPBmCO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "K-bx0rrue1G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "# 18418\n",
        "log(0.01) / log(0.99975)\n",
        "\n",
        "18418 / 100_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcfUwu7F5yEP",
        "outputId": "7e6fecd6-0590-4559-9c97-c088b4527419"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18418"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, features_dim=256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[2]  # Should be 2 for (9, 9, 2) input\n",
        "\n",
        "        # Define a custom CNN architecture\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the CNN dynamically\n",
        "        with th.no_grad():\n",
        "            sample_input = th.as_tensor(observation_space.sample()[None]).float().permute(0, 3, 1, 2)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "            print(\"Flattened output size after CNN:\", n_flatten)  # Debugging statement\n",
        "\n",
        "        # Define fully connected layers using the computed flattened size\n",
        "        # self.linear = nn.Sequential(\n",
        "        #     nn.Linear(n_flatten, 256),  # Use dynamically calculated n_flatten\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, 128),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, features_dim)  # Output size of features_dim (256)\n",
        "        # )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        observations = observations.permute(0, 3, 1, 2)  # Rearrange dimensions for Conv2d\n",
        "        cnn_output = self.cnn(observations)\n",
        "        return self.linear(cnn_output)\n"
      ],
      "metadata": {
        "id": "Mo6iNjTC9dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746bb4bf-2453-46c3-b268-b9f5ef45b677"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "\n",
        "vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1, env_kwargs=dict(end_on_bomb=True))\n",
        "\n",
        "# vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "# vec_env = VecTransposeImage(vec_env)  # Transpose to [batch_size, channels, height, width]\n",
        "\n",
        "# Optimized hyperparameters for DQN\n",
        "learning_rate = 1e-4           # Learning rate for weight updates\n",
        "buffer_size = 500_000            # Replay buffer size\n",
        "learning_starts = 1000         # Steps before learning begins\n",
        "batch_size = 64                # Number of samples per training update\n",
        "train_freq = (4, 'step')    # Frequency of training updates\n",
        "# train_freq = (1, 'episode')\n",
        "target_update_interval = 750   # Steps between target network updates\n",
        "exploration_fraction = 0.25    # Fraction of total timesteps for epsilon decay\n",
        "exploration_final_eps = 0.01   # Final epsilon value after decay\n",
        "gamma = 0.99                    # Discount factor for future rewards\n",
        "\n",
        "#  0.99975 ** x = 0.01 => x = log(0.01) / log(0.99975)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        "    features_extractor_kwargs=dict(features_dim=256)  # Output size of the final layer\n",
        ")\n",
        "\n",
        "# Instantiate DQN with improved hyperparameters\n",
        "model = DQN(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    learning_rate=learning_rate,\n",
        "    buffer_size=buffer_size,\n",
        "    learning_starts=learning_starts,\n",
        "    batch_size=batch_size,\n",
        "    train_freq=train_freq,\n",
        "    target_update_interval=target_update_interval,\n",
        "    exploration_fraction=exploration_fraction,\n",
        "    exploration_final_eps=exploration_final_eps,\n",
        "    gamma=gamma,\n",
        "    policy_kwargs=policy_kwargs,  # Use custom network\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500_000)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cby_Zfh7AA74",
        "outputId": "33f0bbbf-190c-4afe-c5a9-e59ac9fd5202"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00342  |\n",
            "|    n_updates        | 113283   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18220    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 982      |\n",
            "|    total_timesteps  | 454265   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000982 |\n",
            "|    n_updates        | 113316   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18224    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 983      |\n",
            "|    total_timesteps  | 454400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0017   |\n",
            "|    n_updates        | 113349   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18228    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 983      |\n",
            "|    total_timesteps  | 454535   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00143  |\n",
            "|    n_updates        | 113383   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.58     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18232    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 983      |\n",
            "|    total_timesteps  | 454661   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00243  |\n",
            "|    n_updates        | 113415   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18236    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 983      |\n",
            "|    total_timesteps  | 454793   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00356  |\n",
            "|    n_updates        | 113448   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.71     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18240    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 984      |\n",
            "|    total_timesteps  | 454928   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00375  |\n",
            "|    n_updates        | 113481   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.72     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18244    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 984      |\n",
            "|    total_timesteps  | 455061   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00288  |\n",
            "|    n_updates        | 113515   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18248    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 984      |\n",
            "|    total_timesteps  | 455193   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00192  |\n",
            "|    n_updates        | 113548   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18252    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 985      |\n",
            "|    total_timesteps  | 455325   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00143  |\n",
            "|    n_updates        | 113581   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18256    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 985      |\n",
            "|    total_timesteps  | 455457   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0011   |\n",
            "|    n_updates        | 113614   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.83     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18260    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 985      |\n",
            "|    total_timesteps  | 455590   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00109  |\n",
            "|    n_updates        | 113647   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18264    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 985      |\n",
            "|    total_timesteps  | 455714   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00189  |\n",
            "|    n_updates        | 113678   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18268    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 986      |\n",
            "|    total_timesteps  | 455847   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00115  |\n",
            "|    n_updates        | 113711   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18272    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 986      |\n",
            "|    total_timesteps  | 455979   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00215  |\n",
            "|    n_updates        | 113744   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18276    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 986      |\n",
            "|    total_timesteps  | 456114   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00484  |\n",
            "|    n_updates        | 113778   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.81     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18280    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 987      |\n",
            "|    total_timesteps  | 456244   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00209  |\n",
            "|    n_updates        | 113810   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.81     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18284    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 987      |\n",
            "|    total_timesteps  | 456376   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00179  |\n",
            "|    n_updates        | 113843   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.82     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18288    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 987      |\n",
            "|    total_timesteps  | 456508   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000769 |\n",
            "|    n_updates        | 113876   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18292    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 988      |\n",
            "|    total_timesteps  | 456633   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00147  |\n",
            "|    n_updates        | 113908   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18296    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 988      |\n",
            "|    total_timesteps  | 456767   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00418  |\n",
            "|    n_updates        | 113941   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18300    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 988      |\n",
            "|    total_timesteps  | 456901   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00107  |\n",
            "|    n_updates        | 113975   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18304    |\n",
            "|    fps              | 462      |\n",
            "|    time_elapsed     | 989      |\n",
            "|    total_timesteps  | 457033   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000738 |\n",
            "|    n_updates        | 114008   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18308    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 989      |\n",
            "|    total_timesteps  | 457164   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00162  |\n",
            "|    n_updates        | 114040   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18312    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 989      |\n",
            "|    total_timesteps  | 457297   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00176  |\n",
            "|    n_updates        | 114074   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18316    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 990      |\n",
            "|    total_timesteps  | 457431   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0041   |\n",
            "|    n_updates        | 114107   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18320    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 990      |\n",
            "|    total_timesteps  | 457564   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00323  |\n",
            "|    n_updates        | 114140   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18324    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 990      |\n",
            "|    total_timesteps  | 457698   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00248  |\n",
            "|    n_updates        | 114174   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18328    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 991      |\n",
            "|    total_timesteps  | 457831   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00176  |\n",
            "|    n_updates        | 114207   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18332    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 991      |\n",
            "|    total_timesteps  | 457966   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00144  |\n",
            "|    n_updates        | 114241   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18336    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 991      |\n",
            "|    total_timesteps  | 458099   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00174  |\n",
            "|    n_updates        | 114274   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18340    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 992      |\n",
            "|    total_timesteps  | 458233   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00108  |\n",
            "|    n_updates        | 114308   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18344    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 992      |\n",
            "|    total_timesteps  | 458366   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00391  |\n",
            "|    n_updates        | 114341   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18348    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 992      |\n",
            "|    total_timesteps  | 458499   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00426  |\n",
            "|    n_updates        | 114374   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18352    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 993      |\n",
            "|    total_timesteps  | 458633   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00119  |\n",
            "|    n_updates        | 114408   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18356    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 993      |\n",
            "|    total_timesteps  | 458767   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00185  |\n",
            "|    n_updates        | 114441   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18360    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 993      |\n",
            "|    total_timesteps  | 458899   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00582  |\n",
            "|    n_updates        | 114474   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18364    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 993      |\n",
            "|    total_timesteps  | 459028   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00457  |\n",
            "|    n_updates        | 114506   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18368    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 994      |\n",
            "|    total_timesteps  | 459161   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00162  |\n",
            "|    n_updates        | 114540   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.8      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18372    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 994      |\n",
            "|    total_timesteps  | 459295   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00378  |\n",
            "|    n_updates        | 114573   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18376    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 994      |\n",
            "|    total_timesteps  | 459411   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00329  |\n",
            "|    n_updates        | 114602   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18380    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 994      |\n",
            "|    total_timesteps  | 459538   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00184  |\n",
            "|    n_updates        | 114634   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18384    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 995      |\n",
            "|    total_timesteps  | 459673   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00281  |\n",
            "|    n_updates        | 114668   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.71     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18388    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 995      |\n",
            "|    total_timesteps  | 459800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00252  |\n",
            "|    n_updates        | 114699   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18392    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 995      |\n",
            "|    total_timesteps  | 459933   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00154  |\n",
            "|    n_updates        | 114733   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18396    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 996      |\n",
            "|    total_timesteps  | 460065   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00186  |\n",
            "|    n_updates        | 114766   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18400    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 996      |\n",
            "|    total_timesteps  | 460197   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00195  |\n",
            "|    n_updates        | 114799   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18404    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 996      |\n",
            "|    total_timesteps  | 460301   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00349  |\n",
            "|    n_updates        | 114825   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18408    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 996      |\n",
            "|    total_timesteps  | 460433   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00109  |\n",
            "|    n_updates        | 114858   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.63     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18412    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 997      |\n",
            "|    total_timesteps  | 460564   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00172  |\n",
            "|    n_updates        | 114890   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18416    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 997      |\n",
            "|    total_timesteps  | 460699   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00226  |\n",
            "|    n_updates        | 114924   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18420    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 997      |\n",
            "|    total_timesteps  | 460830   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00216  |\n",
            "|    n_updates        | 114957   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.63     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18424    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 997      |\n",
            "|    total_timesteps  | 460957   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00271  |\n",
            "|    n_updates        | 114989   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.63     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18428    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 998      |\n",
            "|    total_timesteps  | 461089   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00116  |\n",
            "|    n_updates        | 115022   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18432    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 998      |\n",
            "|    total_timesteps  | 461221   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00355  |\n",
            "|    n_updates        | 115055   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18436    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 998      |\n",
            "|    total_timesteps  | 461351   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000968 |\n",
            "|    n_updates        | 115087   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18440    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 998      |\n",
            "|    total_timesteps  | 461481   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00137  |\n",
            "|    n_updates        | 115120   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.63     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18444    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 999      |\n",
            "|    total_timesteps  | 461613   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00236  |\n",
            "|    n_updates        | 115153   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18448    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 999      |\n",
            "|    total_timesteps  | 461747   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000707 |\n",
            "|    n_updates        | 115186   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18452    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 999      |\n",
            "|    total_timesteps  | 461880   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0074   |\n",
            "|    n_updates        | 115219   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18456    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1000     |\n",
            "|    total_timesteps  | 462013   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00186  |\n",
            "|    n_updates        | 115253   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18460    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1000     |\n",
            "|    total_timesteps  | 462145   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000952 |\n",
            "|    n_updates        | 115286   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18464    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1000     |\n",
            "|    total_timesteps  | 462279   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000906 |\n",
            "|    n_updates        | 115319   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18468    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1001     |\n",
            "|    total_timesteps  | 462413   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00133  |\n",
            "|    n_updates        | 115353   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18472    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1001     |\n",
            "|    total_timesteps  | 462547   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00205  |\n",
            "|    n_updates        | 115386   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.72     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18476    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1001     |\n",
            "|    total_timesteps  | 462684   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00295  |\n",
            "|    n_updates        | 115420   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18480    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1002     |\n",
            "|    total_timesteps  | 462816   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00241  |\n",
            "|    n_updates        | 115453   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18484    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1002     |\n",
            "|    total_timesteps  | 462948   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00148  |\n",
            "|    n_updates        | 115486   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18488    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1002     |\n",
            "|    total_timesteps  | 463081   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00145  |\n",
            "|    n_updates        | 115520   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18492    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1003     |\n",
            "|    total_timesteps  | 463213   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00408  |\n",
            "|    n_updates        | 115553   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18496    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1003     |\n",
            "|    total_timesteps  | 463344   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00452  |\n",
            "|    n_updates        | 115585   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18500    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1003     |\n",
            "|    total_timesteps  | 463473   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00392  |\n",
            "|    n_updates        | 115618   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.86     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18504    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1004     |\n",
            "|    total_timesteps  | 463605   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00144  |\n",
            "|    n_updates        | 115651   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.87     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18508    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1004     |\n",
            "|    total_timesteps  | 463740   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0013   |\n",
            "|    n_updates        | 115684   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18512    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1004     |\n",
            "|    total_timesteps  | 463849   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00238  |\n",
            "|    n_updates        | 115712   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18516    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1005     |\n",
            "|    total_timesteps  | 463980   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00132  |\n",
            "|    n_updates        | 115744   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18520    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1005     |\n",
            "|    total_timesteps  | 464114   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0017   |\n",
            "|    n_updates        | 115778   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18524    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1005     |\n",
            "|    total_timesteps  | 464244   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0012   |\n",
            "|    n_updates        | 115810   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18528    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1005     |\n",
            "|    total_timesteps  | 464380   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00224  |\n",
            "|    n_updates        | 115844   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18532    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1006     |\n",
            "|    total_timesteps  | 464513   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00115  |\n",
            "|    n_updates        | 115878   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18536    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1006     |\n",
            "|    total_timesteps  | 464636   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0032   |\n",
            "|    n_updates        | 115908   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18540    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1006     |\n",
            "|    total_timesteps  | 464769   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00188  |\n",
            "|    n_updates        | 115942   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18544    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1006     |\n",
            "|    total_timesteps  | 464902   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00192  |\n",
            "|    n_updates        | 115975   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18548    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1007     |\n",
            "|    total_timesteps  | 465034   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00139  |\n",
            "|    n_updates        | 116008   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18552    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1007     |\n",
            "|    total_timesteps  | 465171   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00571  |\n",
            "|    n_updates        | 116042   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18556    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1007     |\n",
            "|    total_timesteps  | 465303   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00314  |\n",
            "|    n_updates        | 116075   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18560    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1008     |\n",
            "|    total_timesteps  | 465433   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000968 |\n",
            "|    n_updates        | 116108   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.71     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18564    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1008     |\n",
            "|    total_timesteps  | 465555   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00121  |\n",
            "|    n_updates        | 116138   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18568    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1008     |\n",
            "|    total_timesteps  | 465687   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00161  |\n",
            "|    n_updates        | 116171   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18572    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1008     |\n",
            "|    total_timesteps  | 465794   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00411  |\n",
            "|    n_updates        | 116198   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18576    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1009     |\n",
            "|    total_timesteps  | 465926   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00202  |\n",
            "|    n_updates        | 116231   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18580    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1009     |\n",
            "|    total_timesteps  | 466058   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00305  |\n",
            "|    n_updates        | 116264   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18584    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1009     |\n",
            "|    total_timesteps  | 466192   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00129  |\n",
            "|    n_updates        | 116297   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18588    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1009     |\n",
            "|    total_timesteps  | 466324   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00291  |\n",
            "|    n_updates        | 116330   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18592    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1010     |\n",
            "|    total_timesteps  | 466447   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00172  |\n",
            "|    n_updates        | 116361   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.59     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18596    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1010     |\n",
            "|    total_timesteps  | 466578   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00172  |\n",
            "|    n_updates        | 116394   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18600    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1010     |\n",
            "|    total_timesteps  | 466715   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00257  |\n",
            "|    n_updates        | 116428   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18604    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1011     |\n",
            "|    total_timesteps  | 466846   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00135  |\n",
            "|    n_updates        | 116461   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.4     |\n",
            "|    ep_rew_mean      | 9.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18608    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1011     |\n",
            "|    total_timesteps  | 466979   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00148  |\n",
            "|    n_updates        | 116494   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18612    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1011     |\n",
            "|    total_timesteps  | 467111   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00132  |\n",
            "|    n_updates        | 116527   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18616    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1011     |\n",
            "|    total_timesteps  | 467241   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00128  |\n",
            "|    n_updates        | 116560   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18620    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1012     |\n",
            "|    total_timesteps  | 467373   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00248  |\n",
            "|    n_updates        | 116593   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18624    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1012     |\n",
            "|    total_timesteps  | 467505   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.002    |\n",
            "|    n_updates        | 116626   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18628    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1012     |\n",
            "|    total_timesteps  | 467637   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00155  |\n",
            "|    n_updates        | 116659   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18632    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1013     |\n",
            "|    total_timesteps  | 467770   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00327  |\n",
            "|    n_updates        | 116692   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18636    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1013     |\n",
            "|    total_timesteps  | 467900   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00216  |\n",
            "|    n_updates        | 116724   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18640    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1013     |\n",
            "|    total_timesteps  | 468032   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00144  |\n",
            "|    n_updates        | 116757   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18644    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1014     |\n",
            "|    total_timesteps  | 468164   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0016   |\n",
            "|    n_updates        | 116790   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18648    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1014     |\n",
            "|    total_timesteps  | 468297   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0054   |\n",
            "|    n_updates        | 116824   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18652    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1014     |\n",
            "|    total_timesteps  | 468433   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00127  |\n",
            "|    n_updates        | 116858   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18656    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1015     |\n",
            "|    total_timesteps  | 468566   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00261  |\n",
            "|    n_updates        | 116891   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18660    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1015     |\n",
            "|    total_timesteps  | 468699   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00122  |\n",
            "|    n_updates        | 116924   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18664    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1015     |\n",
            "|    total_timesteps  | 468835   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00207  |\n",
            "|    n_updates        | 116958   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.73     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18668    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1016     |\n",
            "|    total_timesteps  | 468967   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00297  |\n",
            "|    n_updates        | 116991   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.83     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18672    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1016     |\n",
            "|    total_timesteps  | 469097   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00309  |\n",
            "|    n_updates        | 117024   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.83     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18676    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1016     |\n",
            "|    total_timesteps  | 469229   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0013   |\n",
            "|    n_updates        | 117057   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.8      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18680    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1017     |\n",
            "|    total_timesteps  | 469356   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00245  |\n",
            "|    n_updates        | 117088   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18684    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1017     |\n",
            "|    total_timesteps  | 469460   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00164  |\n",
            "|    n_updates        | 117114   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18688    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1017     |\n",
            "|    total_timesteps  | 469570   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0017   |\n",
            "|    n_updates        | 117142   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18692    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1017     |\n",
            "|    total_timesteps  | 469697   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00143  |\n",
            "|    n_updates        | 117174   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18696    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1018     |\n",
            "|    total_timesteps  | 469829   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0015   |\n",
            "|    n_updates        | 117207   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18700    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1018     |\n",
            "|    total_timesteps  | 469933   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00053  |\n",
            "|    n_updates        | 117233   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.53     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18704    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1018     |\n",
            "|    total_timesteps  | 470065   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000817 |\n",
            "|    n_updates        | 117266   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18708    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1018     |\n",
            "|    total_timesteps  | 470200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00169  |\n",
            "|    n_updates        | 117299   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18712    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1019     |\n",
            "|    total_timesteps  | 470329   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00139  |\n",
            "|    n_updates        | 117332   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.56     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18716    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1019     |\n",
            "|    total_timesteps  | 470461   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00182  |\n",
            "|    n_updates        | 117365   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18720    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1019     |\n",
            "|    total_timesteps  | 470596   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00258  |\n",
            "|    n_updates        | 117398   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18724    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1019     |\n",
            "|    total_timesteps  | 470731   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00242  |\n",
            "|    n_updates        | 117432   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.53     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18728    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1020     |\n",
            "|    total_timesteps  | 470860   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00311  |\n",
            "|    n_updates        | 117464   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.52     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18732    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1020     |\n",
            "|    total_timesteps  | 470995   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00236  |\n",
            "|    n_updates        | 117498   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18736    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1020     |\n",
            "|    total_timesteps  | 471128   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0015   |\n",
            "|    n_updates        | 117531   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18740    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 471261   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00186  |\n",
            "|    n_updates        | 117565   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18744    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 471396   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00207  |\n",
            "|    n_updates        | 117598   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.52     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18748    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 471522   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00136  |\n",
            "|    n_updates        | 117630   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.53     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18752    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 471659   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00161  |\n",
            "|    n_updates        | 117664   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.5      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18756    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1022     |\n",
            "|    total_timesteps  | 471784   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000859 |\n",
            "|    n_updates        | 117695   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.5      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18760    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1022     |\n",
            "|    total_timesteps  | 471919   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00124  |\n",
            "|    n_updates        | 117729   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18764    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1022     |\n",
            "|    total_timesteps  | 472053   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00387  |\n",
            "|    n_updates        | 117763   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18768    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1023     |\n",
            "|    total_timesteps  | 472185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00132  |\n",
            "|    n_updates        | 117796   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.52     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18772    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1023     |\n",
            "|    total_timesteps  | 472319   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00103  |\n",
            "|    n_updates        | 117829   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.2     |\n",
            "|    ep_rew_mean      | 9.53     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18776    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1023     |\n",
            "|    total_timesteps  | 472453   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00218  |\n",
            "|    n_updates        | 117863   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.56     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18780    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1023     |\n",
            "|    total_timesteps  | 472587   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00499  |\n",
            "|    n_updates        | 117896   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.6     |\n",
            "|    ep_rew_mean      | 9.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18784    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1024     |\n",
            "|    total_timesteps  | 472722   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00405  |\n",
            "|    n_updates        | 117930   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18788    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1024     |\n",
            "|    total_timesteps  | 472857   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00121  |\n",
            "|    n_updates        | 117964   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18792    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1024     |\n",
            "|    total_timesteps  | 472990   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00101  |\n",
            "|    n_updates        | 117997   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18796    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1025     |\n",
            "|    total_timesteps  | 473126   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00641  |\n",
            "|    n_updates        | 118031   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.3     |\n",
            "|    ep_rew_mean      | 9.9      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18800    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1025     |\n",
            "|    total_timesteps  | 473262   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00367  |\n",
            "|    n_updates        | 118065   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.4     |\n",
            "|    ep_rew_mean      | 9.9      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18804    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1025     |\n",
            "|    total_timesteps  | 473400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00197  |\n",
            "|    n_updates        | 118099   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.8      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18808    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1025     |\n",
            "|    total_timesteps  | 473511   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00678  |\n",
            "|    n_updates        | 118127   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18812    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1026     |\n",
            "|    total_timesteps  | 473641   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00179  |\n",
            "|    n_updates        | 118160   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18816    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1026     |\n",
            "|    total_timesteps  | 473775   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00249  |\n",
            "|    n_updates        | 118193   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18820    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1026     |\n",
            "|    total_timesteps  | 473907   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00128  |\n",
            "|    n_updates        | 118226   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18824    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1027     |\n",
            "|    total_timesteps  | 474043   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00305  |\n",
            "|    n_updates        | 118260   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18828    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1027     |\n",
            "|    total_timesteps  | 474178   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00229  |\n",
            "|    n_updates        | 118294   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18832    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1028     |\n",
            "|    total_timesteps  | 474308   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00655  |\n",
            "|    n_updates        | 118326   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18836    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1028     |\n",
            "|    total_timesteps  | 474447   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00151  |\n",
            "|    n_updates        | 118361   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.8      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18840    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1028     |\n",
            "|    total_timesteps  | 474583   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00223  |\n",
            "|    n_updates        | 118395   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.81     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18844    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1029     |\n",
            "|    total_timesteps  | 474717   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00212  |\n",
            "|    n_updates        | 118429   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.3     |\n",
            "|    ep_rew_mean      | 9.84     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18848    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1029     |\n",
            "|    total_timesteps  | 474851   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00345  |\n",
            "|    n_updates        | 118462   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.72     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18852    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1029     |\n",
            "|    total_timesteps  | 474955   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00258  |\n",
            "|    n_updates        | 118488   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18856    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1029     |\n",
            "|    total_timesteps  | 475092   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00211  |\n",
            "|    n_updates        | 118522   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18860    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1030     |\n",
            "|    total_timesteps  | 475228   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.002    |\n",
            "|    n_updates        | 118556   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.77     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18864    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1030     |\n",
            "|    total_timesteps  | 475364   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00295  |\n",
            "|    n_updates        | 118590   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.3     |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18868    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1030     |\n",
            "|    total_timesteps  | 475516   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00186  |\n",
            "|    n_updates        | 118628   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.3     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18872    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1031     |\n",
            "|    total_timesteps  | 475647   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00137  |\n",
            "|    n_updates        | 118661   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.3     |\n",
            "|    ep_rew_mean      | 9.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18876    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1031     |\n",
            "|    total_timesteps  | 475783   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.003    |\n",
            "|    n_updates        | 118695   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18880    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1031     |\n",
            "|    total_timesteps  | 475904   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00192  |\n",
            "|    n_updates        | 118725   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18884    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1031     |\n",
            "|    total_timesteps  | 476038   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00204  |\n",
            "|    n_updates        | 118759   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18888    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1032     |\n",
            "|    total_timesteps  | 476151   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00196  |\n",
            "|    n_updates        | 118787   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18892    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1032     |\n",
            "|    total_timesteps  | 476284   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00168  |\n",
            "|    n_updates        | 118820   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.44     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18896    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1032     |\n",
            "|    total_timesteps  | 476420   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00184  |\n",
            "|    n_updates        | 118854   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.45     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18900    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1033     |\n",
            "|    total_timesteps  | 476555   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00167  |\n",
            "|    n_updates        | 118888   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.42     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18904    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1033     |\n",
            "|    total_timesteps  | 476684   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000936 |\n",
            "|    n_updates        | 118920   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.48     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18908    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1033     |\n",
            "|    total_timesteps  | 476815   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00193  |\n",
            "|    n_updates        | 118953   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18912    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1033     |\n",
            "|    total_timesteps  | 476950   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00387  |\n",
            "|    n_updates        | 118987   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.47     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18916    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1034     |\n",
            "|    total_timesteps  | 477083   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00162  |\n",
            "|    n_updates        | 119020   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.49     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18920    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1034     |\n",
            "|    total_timesteps  | 477218   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00288  |\n",
            "|    n_updates        | 119054   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.47     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18924    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1034     |\n",
            "|    total_timesteps  | 477344   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00276  |\n",
            "|    n_updates        | 119085   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.47     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18928    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1034     |\n",
            "|    total_timesteps  | 477477   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00297  |\n",
            "|    n_updates        | 119119   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.5      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18932    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1035     |\n",
            "|    total_timesteps  | 477612   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00361  |\n",
            "|    n_updates        | 119152   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.48     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18936    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1035     |\n",
            "|    total_timesteps  | 477743   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0027   |\n",
            "|    n_updates        | 119185   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18940    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1035     |\n",
            "|    total_timesteps  | 477876   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00196  |\n",
            "|    n_updates        | 119218   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18944    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1036     |\n",
            "|    total_timesteps  | 478009   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000884 |\n",
            "|    n_updates        | 119252   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18948    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1036     |\n",
            "|    total_timesteps  | 478145   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00227  |\n",
            "|    n_updates        | 119286   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.2     |\n",
            "|    ep_rew_mean      | 9.59     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18952    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1036     |\n",
            "|    total_timesteps  | 478278   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0012   |\n",
            "|    n_updates        | 119319   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18956    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1036     |\n",
            "|    total_timesteps  | 478406   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00122  |\n",
            "|    n_updates        | 119351   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18960    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1037     |\n",
            "|    total_timesteps  | 478537   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00163  |\n",
            "|    n_updates        | 119384   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.53     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18964    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1037     |\n",
            "|    total_timesteps  | 478674   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00754  |\n",
            "|    n_updates        | 119418   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18968    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1037     |\n",
            "|    total_timesteps  | 478812   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00196  |\n",
            "|    n_updates        | 119452   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.9     |\n",
            "|    ep_rew_mean      | 9.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18972    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1037     |\n",
            "|    total_timesteps  | 478941   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00193  |\n",
            "|    n_updates        | 119485   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18976    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1038     |\n",
            "|    total_timesteps  | 479078   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00189  |\n",
            "|    n_updates        | 119519   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.71     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18980    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1038     |\n",
            "|    total_timesteps  | 479212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00348  |\n",
            "|    n_updates        | 119552   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.1     |\n",
            "|    ep_rew_mean      | 9.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18984    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1038     |\n",
            "|    total_timesteps  | 479347   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00191  |\n",
            "|    n_updates        | 119586   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.4      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18988    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1039     |\n",
            "|    total_timesteps  | 479701   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00841  |\n",
            "|    n_updates        | 119675   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.4      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18992    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1040     |\n",
            "|    total_timesteps  | 479836   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00179  |\n",
            "|    n_updates        | 119708   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.41     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 18996    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1040     |\n",
            "|    total_timesteps  | 479973   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00228  |\n",
            "|    n_updates        | 119743   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.41     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19000    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1041     |\n",
            "|    total_timesteps  | 480108   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00232  |\n",
            "|    n_updates        | 119776   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.43     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19004    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1041     |\n",
            "|    total_timesteps  | 480245   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00275  |\n",
            "|    n_updates        | 119811   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19008    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1041     |\n",
            "|    total_timesteps  | 480380   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00271  |\n",
            "|    n_updates        | 119844   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.7     |\n",
            "|    ep_rew_mean      | 8.46     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19012    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1042     |\n",
            "|    total_timesteps  | 480518   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0016   |\n",
            "|    n_updates        | 119879   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.7     |\n",
            "|    ep_rew_mean      | 8.49     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19016    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1042     |\n",
            "|    total_timesteps  | 480650   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0027   |\n",
            "|    n_updates        | 119912   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.49     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19020    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1042     |\n",
            "|    total_timesteps  | 480783   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00234  |\n",
            "|    n_updates        | 119945   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.7     |\n",
            "|    ep_rew_mean      | 8.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19024    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1042     |\n",
            "|    total_timesteps  | 480915   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00169  |\n",
            "|    n_updates        | 119978   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.41     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19028    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1043     |\n",
            "|    total_timesteps  | 481023   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00328  |\n",
            "|    n_updates        | 120005   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.4     |\n",
            "|    ep_rew_mean      | 8.38     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19032    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1043     |\n",
            "|    total_timesteps  | 481155   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00169  |\n",
            "|    n_updates        | 120038   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.34     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19036    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1043     |\n",
            "|    total_timesteps  | 481292   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00208  |\n",
            "|    n_updates        | 120072   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.5     |\n",
            "|    ep_rew_mean      | 8.33     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19040    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1043     |\n",
            "|    total_timesteps  | 481424   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00472  |\n",
            "|    n_updates        | 120105   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.14     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19044    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1044     |\n",
            "|    total_timesteps  | 481571   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00214  |\n",
            "|    n_updates        | 120142   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19048    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1044     |\n",
            "|    total_timesteps  | 481707   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00155  |\n",
            "|    n_updates        | 120176   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.6     |\n",
            "|    ep_rew_mean      | 8.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19052    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1044     |\n",
            "|    total_timesteps  | 481840   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0042   |\n",
            "|    n_updates        | 120209   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.7     |\n",
            "|    ep_rew_mean      | 8.17     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19056    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1045     |\n",
            "|    total_timesteps  | 481976   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0083   |\n",
            "|    n_updates        | 120243   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.8     |\n",
            "|    ep_rew_mean      | 8.19     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19060    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1045     |\n",
            "|    total_timesteps  | 482112   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00189  |\n",
            "|    n_updates        | 120277   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.7     |\n",
            "|    ep_rew_mean      | 8.18     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19064    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1045     |\n",
            "|    total_timesteps  | 482246   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00573  |\n",
            "|    n_updates        | 120311   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.1     |\n",
            "|    ep_rew_mean      | 7.86     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19068    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1045     |\n",
            "|    total_timesteps  | 482423   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0026   |\n",
            "|    n_updates        | 120355   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.1     |\n",
            "|    ep_rew_mean      | 7.88     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19072    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1046     |\n",
            "|    total_timesteps  | 482556   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00616  |\n",
            "|    n_updates        | 120388   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.1     |\n",
            "|    ep_rew_mean      | 7.89     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19076    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1046     |\n",
            "|    total_timesteps  | 482688   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00149  |\n",
            "|    n_updates        | 120421   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36       |\n",
            "|    ep_rew_mean      | 7.82     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19080    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1046     |\n",
            "|    total_timesteps  | 482812   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00134  |\n",
            "|    n_updates        | 120452   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36       |\n",
            "|    ep_rew_mean      | 7.81     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19084    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1047     |\n",
            "|    total_timesteps  | 482946   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00152  |\n",
            "|    n_updates        | 120486   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.8     |\n",
            "|    ep_rew_mean      | 9.16     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19088    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1047     |\n",
            "|    total_timesteps  | 483079   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00817  |\n",
            "|    n_updates        | 120519   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.8     |\n",
            "|    ep_rew_mean      | 9.14     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19092    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1047     |\n",
            "|    total_timesteps  | 483212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00234  |\n",
            "|    n_updates        | 120552   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.7     |\n",
            "|    ep_rew_mean      | 9.1      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19096    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1047     |\n",
            "|    total_timesteps  | 483343   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0025   |\n",
            "|    n_updates        | 120585   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.7     |\n",
            "|    ep_rew_mean      | 9.1      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19100    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1048     |\n",
            "|    total_timesteps  | 483480   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00126  |\n",
            "|    n_updates        | 120619   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.5     |\n",
            "|    ep_rew_mean      | 9.04     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19104    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1048     |\n",
            "|    total_timesteps  | 483599   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00191  |\n",
            "|    n_updates        | 120649   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.5     |\n",
            "|    ep_rew_mean      | 9.02     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19108    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1048     |\n",
            "|    total_timesteps  | 483734   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00324  |\n",
            "|    n_updates        | 120683   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.5     |\n",
            "|    ep_rew_mean      | 9.04     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19112    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1048     |\n",
            "|    total_timesteps  | 483871   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00358  |\n",
            "|    n_updates        | 120717   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 9.05     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19116    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1049     |\n",
            "|    total_timesteps  | 484009   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000829 |\n",
            "|    n_updates        | 120752   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 9.06     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19120    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1049     |\n",
            "|    total_timesteps  | 484145   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00232  |\n",
            "|    n_updates        | 120786   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.9     |\n",
            "|    ep_rew_mean      | 8.39     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19124    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1050     |\n",
            "|    total_timesteps  | 484405   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00238  |\n",
            "|    n_updates        | 120851   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.2     |\n",
            "|    ep_rew_mean      | 8.48     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19128    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1050     |\n",
            "|    total_timesteps  | 484539   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000982 |\n",
            "|    n_updates        | 120884   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.2     |\n",
            "|    ep_rew_mean      | 8.51     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19132    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1050     |\n",
            "|    total_timesteps  | 484674   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00301  |\n",
            "|    n_updates        | 120918   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.2     |\n",
            "|    ep_rew_mean      | 8.56     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19136    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1050     |\n",
            "|    total_timesteps  | 484809   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0016   |\n",
            "|    n_updates        | 120952   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.2     |\n",
            "|    ep_rew_mean      | 8.05     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19140    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1051     |\n",
            "|    total_timesteps  | 485045   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00271  |\n",
            "|    n_updates        | 121011   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.1     |\n",
            "|    ep_rew_mean      | 8.25     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19144    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1051     |\n",
            "|    total_timesteps  | 485180   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00333  |\n",
            "|    n_updates        | 121044   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.3     |\n",
            "|    ep_rew_mean      | 8.16     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19148    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1052     |\n",
            "|    total_timesteps  | 485339   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00133  |\n",
            "|    n_updates        | 121084   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.8     |\n",
            "|    ep_rew_mean      | 7.93     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19152    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1052     |\n",
            "|    total_timesteps  | 485518   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00111  |\n",
            "|    n_updates        | 121129   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.8     |\n",
            "|    ep_rew_mean      | 7.93     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19156    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1053     |\n",
            "|    total_timesteps  | 485654   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00323  |\n",
            "|    n_updates        | 121163   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.8     |\n",
            "|    ep_rew_mean      | 7.93     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19160    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1053     |\n",
            "|    total_timesteps  | 485790   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00509  |\n",
            "|    n_updates        | 121197   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.8     |\n",
            "|    ep_rew_mean      | 7.94     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19164    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1053     |\n",
            "|    total_timesteps  | 485925   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00195  |\n",
            "|    n_updates        | 121231   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.4     |\n",
            "|    ep_rew_mean      | 7.7      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19168    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1054     |\n",
            "|    total_timesteps  | 486162   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00284  |\n",
            "|    n_updates        | 121290   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.4     |\n",
            "|    ep_rew_mean      | 7.71     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19172    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1054     |\n",
            "|    total_timesteps  | 486299   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0105   |\n",
            "|    n_updates        | 121324   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.9     |\n",
            "|    ep_rew_mean      | 6.95     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19176    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1055     |\n",
            "|    total_timesteps  | 486580   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00272  |\n",
            "|    n_updates        | 121394   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 40.4     |\n",
            "|    ep_rew_mean      | 6.29     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19180    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1056     |\n",
            "|    total_timesteps  | 486849   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00254  |\n",
            "|    n_updates        | 121462   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 41.7     |\n",
            "|    ep_rew_mean      | 5.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19184    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1056     |\n",
            "|    total_timesteps  | 487116   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0027   |\n",
            "|    n_updates        | 121528   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 41.7     |\n",
            "|    ep_rew_mean      | 5.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19188    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1056     |\n",
            "|    total_timesteps  | 487250   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00405  |\n",
            "|    n_updates        | 121562   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 41.7     |\n",
            "|    ep_rew_mean      | 5.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19192    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1057     |\n",
            "|    total_timesteps  | 487381   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00121  |\n",
            "|    n_updates        | 121595   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 41.7     |\n",
            "|    ep_rew_mean      | 5.57     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19196    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1057     |\n",
            "|    total_timesteps  | 487509   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0055   |\n",
            "|    n_updates        | 121627   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.3     |\n",
            "|    ep_rew_mean      | 5.26     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19200    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1057     |\n",
            "|    total_timesteps  | 487706   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00259  |\n",
            "|    n_updates        | 121676   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.2     |\n",
            "|    ep_rew_mean      | 4.8      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19204    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1058     |\n",
            "|    total_timesteps  | 487918   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00532  |\n",
            "|    n_updates        | 121729   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.2     |\n",
            "|    ep_rew_mean      | 4.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19208    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1058     |\n",
            "|    total_timesteps  | 488051   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00141  |\n",
            "|    n_updates        | 121762   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 45.1     |\n",
            "|    ep_rew_mean      | 3.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19212    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1059     |\n",
            "|    total_timesteps  | 488386   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00358  |\n",
            "|    n_updates        | 121846   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 45.2     |\n",
            "|    ep_rew_mean      | 3.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19216    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1059     |\n",
            "|    total_timesteps  | 488526   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00187  |\n",
            "|    n_updates        | 121881   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 45.1     |\n",
            "|    ep_rew_mean      | 3.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19220    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1059     |\n",
            "|    total_timesteps  | 488654   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00207  |\n",
            "|    n_updates        | 121913   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.9     |\n",
            "|    ep_rew_mean      | 4.22     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19224    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1060     |\n",
            "|    total_timesteps  | 488790   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00124  |\n",
            "|    n_updates        | 121947   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.9     |\n",
            "|    ep_rew_mean      | 4.22     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19228    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1060     |\n",
            "|    total_timesteps  | 488926   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00375  |\n",
            "|    n_updates        | 121981   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.9     |\n",
            "|    ep_rew_mean      | 4.21     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19232    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1060     |\n",
            "|    total_timesteps  | 489062   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.002    |\n",
            "|    n_updates        | 122015   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 43.8     |\n",
            "|    ep_rew_mean      | 4.14     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19236    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1060     |\n",
            "|    total_timesteps  | 489186   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00201  |\n",
            "|    n_updates        | 122046   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.8     |\n",
            "|    ep_rew_mean      | 4.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19240    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1061     |\n",
            "|    total_timesteps  | 489322   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00317  |\n",
            "|    n_updates        | 122080   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.8     |\n",
            "|    ep_rew_mean      | 4.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19244    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1061     |\n",
            "|    total_timesteps  | 489458   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00324  |\n",
            "|    n_updates        | 122114   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.5     |\n",
            "|    ep_rew_mean      | 4.79     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19248    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1061     |\n",
            "|    total_timesteps  | 489592   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00372  |\n",
            "|    n_updates        | 122147   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.1     |\n",
            "|    ep_rew_mean      | 5.03     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19252    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1062     |\n",
            "|    total_timesteps  | 489727   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00592  |\n",
            "|    n_updates        | 122181   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.1     |\n",
            "|    ep_rew_mean      | 5.03     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19256    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1062     |\n",
            "|    total_timesteps  | 489864   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00622  |\n",
            "|    n_updates        | 122215   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.1     |\n",
            "|    ep_rew_mean      | 5.02     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19260    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1062     |\n",
            "|    total_timesteps  | 489997   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00353  |\n",
            "|    n_updates        | 122249   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.1     |\n",
            "|    ep_rew_mean      | 5.01     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19264    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1062     |\n",
            "|    total_timesteps  | 490131   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00262  |\n",
            "|    n_updates        | 122282   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.4     |\n",
            "|    ep_rew_mean      | 4.86     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19268    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1063     |\n",
            "|    total_timesteps  | 490402   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00201  |\n",
            "|    n_updates        | 122350   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 42.4     |\n",
            "|    ep_rew_mean      | 4.85     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19272    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1063     |\n",
            "|    total_timesteps  | 490538   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00296  |\n",
            "|    n_updates        | 122384   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 41       |\n",
            "|    ep_rew_mean      | 5.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19276    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1064     |\n",
            "|    total_timesteps  | 490675   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00166  |\n",
            "|    n_updates        | 122418   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 39.6     |\n",
            "|    ep_rew_mean      | 6.34     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19280    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1064     |\n",
            "|    total_timesteps  | 490811   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00171  |\n",
            "|    n_updates        | 122452   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.3     |\n",
            "|    ep_rew_mean      | 7.04     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19284    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1064     |\n",
            "|    total_timesteps  | 490947   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00321  |\n",
            "|    n_updates        | 122486   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.3     |\n",
            "|    ep_rew_mean      | 7.07     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19288    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1065     |\n",
            "|    total_timesteps  | 491077   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0034   |\n",
            "|    n_updates        | 122519   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38       |\n",
            "|    ep_rew_mean      | 6.98     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19292    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1065     |\n",
            "|    total_timesteps  | 491183   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00184  |\n",
            "|    n_updates        | 122545   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.9     |\n",
            "|    ep_rew_mean      | 6.99     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19296    |\n",
            "|    fps              | 461      |\n",
            "|    time_elapsed     | 1065     |\n",
            "|    total_timesteps  | 491301   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00342  |\n",
            "|    n_updates        | 122575   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.1     |\n",
            "|    ep_rew_mean      | 7.23     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19300    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1065     |\n",
            "|    total_timesteps  | 491416   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00135  |\n",
            "|    n_updates        | 122603   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.9     |\n",
            "|    ep_rew_mean      | 7.59     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19304    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1066     |\n",
            "|    total_timesteps  | 491507   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00326  |\n",
            "|    n_updates        | 122626   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 35.9     |\n",
            "|    ep_rew_mean      | 7.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19308    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1066     |\n",
            "|    total_timesteps  | 491643   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00241  |\n",
            "|    n_updates        | 122660   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.9     |\n",
            "|    ep_rew_mean      | 8.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19312    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1067     |\n",
            "|    total_timesteps  | 491780   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00254  |\n",
            "|    n_updates        | 122694   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.69     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19316    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1067     |\n",
            "|    total_timesteps  | 491933   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00276  |\n",
            "|    n_updates        | 122733   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19320    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1067     |\n",
            "|    total_timesteps  | 492065   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00847  |\n",
            "|    n_updates        | 122766   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19324    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1068     |\n",
            "|    total_timesteps  | 492202   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00145  |\n",
            "|    n_updates        | 122800   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.76     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19328    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1068     |\n",
            "|    total_timesteps  | 492337   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00386  |\n",
            "|    n_updates        | 122834   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.75     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19332    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1068     |\n",
            "|    total_timesteps  | 492468   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00312  |\n",
            "|    n_updates        | 122866   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.2     |\n",
            "|    ep_rew_mean      | 8.82     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19336    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1068     |\n",
            "|    total_timesteps  | 492605   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00203  |\n",
            "|    n_updates        | 122901   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.78     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19340    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1069     |\n",
            "|    total_timesteps  | 492734   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00122  |\n",
            "|    n_updates        | 122933   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34       |\n",
            "|    ep_rew_mean      | 8.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19344    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1069     |\n",
            "|    total_timesteps  | 492862   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00219  |\n",
            "|    n_updates        | 122965   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.74     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19348    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1069     |\n",
            "|    total_timesteps  | 493000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00099  |\n",
            "|    n_updates        | 122999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.2     |\n",
            "|    ep_rew_mean      | 8.39     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19352    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1070     |\n",
            "|    total_timesteps  | 493143   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000571 |\n",
            "|    n_updates        | 123035   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.37     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19356    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1070     |\n",
            "|    total_timesteps  | 493275   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0017   |\n",
            "|    n_updates        | 123068   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.37     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19360    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1070     |\n",
            "|    total_timesteps  | 493410   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00253  |\n",
            "|    n_updates        | 123102   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 34.1     |\n",
            "|    ep_rew_mean      | 8.39     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19364    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1070     |\n",
            "|    total_timesteps  | 493546   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00243  |\n",
            "|    n_updates        | 123136   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 8.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19368    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1071     |\n",
            "|    total_timesteps  | 493767   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00153  |\n",
            "|    n_updates        | 123191   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 8.58     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19372    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1071     |\n",
            "|    total_timesteps  | 493897   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00271  |\n",
            "|    n_updates        | 123224   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 8.57     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19376    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1071     |\n",
            "|    total_timesteps  | 494031   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00117  |\n",
            "|    n_updates        | 123257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 8.56     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19380    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1072     |\n",
            "|    total_timesteps  | 494167   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00131  |\n",
            "|    n_updates        | 123291   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.5     |\n",
            "|    ep_rew_mean      | 8.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19384    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1072     |\n",
            "|    total_timesteps  | 494301   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00176  |\n",
            "|    n_updates        | 123325   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.6     |\n",
            "|    ep_rew_mean      | 8.57     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19388    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1072     |\n",
            "|    total_timesteps  | 494435   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00283  |\n",
            "|    n_updates        | 123358   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.8     |\n",
            "|    ep_rew_mean      | 8.66     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19392    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1073     |\n",
            "|    total_timesteps  | 494567   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00219  |\n",
            "|    n_updates        | 123391   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.9     |\n",
            "|    ep_rew_mean      | 7.22     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19396    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1073     |\n",
            "|    total_timesteps  | 494993   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00187  |\n",
            "|    n_updates        | 123498   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.6     |\n",
            "|    ep_rew_mean      | 6.52     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19400    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1074     |\n",
            "|    total_timesteps  | 495276   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00156  |\n",
            "|    n_updates        | 123568   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 39       |\n",
            "|    ep_rew_mean      | 6.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19404    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1074     |\n",
            "|    total_timesteps  | 495411   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00114  |\n",
            "|    n_updates        | 123602   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 39       |\n",
            "|    ep_rew_mean      | 6.68     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19408    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 495548   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00612  |\n",
            "|    n_updates        | 123636   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.7     |\n",
            "|    ep_rew_mean      | 6.57     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19412    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 495650   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00276  |\n",
            "|    n_updates        | 123662   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.5     |\n",
            "|    ep_rew_mean      | 6.64     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19416    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 495787   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0021   |\n",
            "|    n_updates        | 123696   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.2     |\n",
            "|    ep_rew_mean      | 6.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19420    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 495890   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00158  |\n",
            "|    n_updates        | 123722   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 38.2     |\n",
            "|    ep_rew_mean      | 6.54     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19424    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1076     |\n",
            "|    total_timesteps  | 496024   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00477  |\n",
            "|    n_updates        | 123755   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.6     |\n",
            "|    ep_rew_mean      | 6.32     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19428    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1076     |\n",
            "|    total_timesteps  | 496094   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00299  |\n",
            "|    n_updates        | 123773   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.6     |\n",
            "|    ep_rew_mean      | 6.35     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19432    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1076     |\n",
            "|    total_timesteps  | 496228   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00144  |\n",
            "|    n_updates        | 123806   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.6     |\n",
            "|    ep_rew_mean      | 6.34     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19436    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1076     |\n",
            "|    total_timesteps  | 496362   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00107  |\n",
            "|    n_updates        | 123840   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.6     |\n",
            "|    ep_rew_mean      | 6.35     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19440    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1077     |\n",
            "|    total_timesteps  | 496497   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00206  |\n",
            "|    n_updates        | 123874   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.7     |\n",
            "|    ep_rew_mean      | 6.38     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19444    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1077     |\n",
            "|    total_timesteps  | 496633   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00233  |\n",
            "|    n_updates        | 123908   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.4     |\n",
            "|    ep_rew_mean      | 6.28     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19448    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1077     |\n",
            "|    total_timesteps  | 496742   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00125  |\n",
            "|    n_updates        | 123935   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.3     |\n",
            "|    ep_rew_mean      | 6.61     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19452    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1078     |\n",
            "|    total_timesteps  | 496875   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00259  |\n",
            "|    n_updates        | 123968   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.3     |\n",
            "|    ep_rew_mean      | 6.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19456    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1078     |\n",
            "|    total_timesteps  | 497008   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00192  |\n",
            "|    n_updates        | 124001   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.4     |\n",
            "|    ep_rew_mean      | 6.6      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19460    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1079     |\n",
            "|    total_timesteps  | 497145   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0033   |\n",
            "|    n_updates        | 124036   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 37.3     |\n",
            "|    ep_rew_mean      | 6.58     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19464    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1079     |\n",
            "|    total_timesteps  | 497279   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00372  |\n",
            "|    n_updates        | 124069   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.5     |\n",
            "|    ep_rew_mean      | 7.03     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19468    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1080     |\n",
            "|    total_timesteps  | 497416   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00103  |\n",
            "|    n_updates        | 124103   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.6     |\n",
            "|    ep_rew_mean      | 7.1      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19472    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1080     |\n",
            "|    total_timesteps  | 497555   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00301  |\n",
            "|    n_updates        | 124138   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.6     |\n",
            "|    ep_rew_mean      | 7.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19476    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1081     |\n",
            "|    total_timesteps  | 497693   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00282  |\n",
            "|    n_updates        | 124173   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.6     |\n",
            "|    ep_rew_mean      | 7.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19480    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1081     |\n",
            "|    total_timesteps  | 497829   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00214  |\n",
            "|    n_updates        | 124207   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.6     |\n",
            "|    ep_rew_mean      | 7.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19484    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1082     |\n",
            "|    total_timesteps  | 497963   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00175  |\n",
            "|    n_updates        | 124240   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.6     |\n",
            "|    ep_rew_mean      | 7.13     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19488    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1082     |\n",
            "|    total_timesteps  | 498098   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00165  |\n",
            "|    n_updates        | 124274   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 36.7     |\n",
            "|    ep_rew_mean      | 7.15     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19492    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1082     |\n",
            "|    total_timesteps  | 498234   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00183  |\n",
            "|    n_updates        | 124308   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33.8     |\n",
            "|    ep_rew_mean      | 8.67     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19496    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1083     |\n",
            "|    total_timesteps  | 498370   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00297  |\n",
            "|    n_updates        | 124342   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.44     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19500    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1083     |\n",
            "|    total_timesteps  | 498506   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000634 |\n",
            "|    n_updates        | 124376   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.45     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19504    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1083     |\n",
            "|    total_timesteps  | 498642   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0012   |\n",
            "|    n_updates        | 124410   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.45     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19508    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1083     |\n",
            "|    total_timesteps  | 498777   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00202  |\n",
            "|    n_updates        | 124444   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.48     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19512    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1084     |\n",
            "|    total_timesteps  | 498898   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00556  |\n",
            "|    n_updates        | 124474   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.5      |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19516    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1084     |\n",
            "|    total_timesteps  | 499035   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00289  |\n",
            "|    n_updates        | 124508   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.62     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19520    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1084     |\n",
            "|    total_timesteps  | 499171   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00371  |\n",
            "|    n_updates        | 124542   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.42     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19524    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1084     |\n",
            "|    total_timesteps  | 499258   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000647 |\n",
            "|    n_updates        | 124564   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19528    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1085     |\n",
            "|    total_timesteps  | 499394   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00179  |\n",
            "|    n_updates        | 124598   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 33       |\n",
            "|    ep_rew_mean      | 9.65     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19532    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1085     |\n",
            "|    total_timesteps  | 499531   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00202  |\n",
            "|    n_updates        | 124632   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.7     |\n",
            "|    ep_rew_mean      | 9.52     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19536    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1085     |\n",
            "|    total_timesteps  | 499636   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00286  |\n",
            "|    n_updates        | 124658   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.8     |\n",
            "|    ep_rew_mean      | 9.55     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19540    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1085     |\n",
            "|    total_timesteps  | 499773   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00232  |\n",
            "|    n_updates        | 124693   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.3     |\n",
            "|    ep_rew_mean      | 9.38     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19544    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1086     |\n",
            "|    total_timesteps  | 499860   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00337  |\n",
            "|    n_updates        | 124714   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 32.5     |\n",
            "|    ep_rew_mean      | 9.48     |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 19548    |\n",
            "|    fps              | 460      |\n",
            "|    time_elapsed     | 1086     |\n",
            "|    total_timesteps  | 499995   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00111  |\n",
            "|    n_updates        | 124748   |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x782e3aef3040>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO"
      ],
      "metadata": {
        "id": "DxHnJuk2e3ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "\n",
        "# Define hyperparameters for PPO\n",
        "learning_rate = 1e-4       # Learning rate for PPO\n",
        "n_steps = 2048             # Number of steps to run for each environment per update\n",
        "batch_size = 64            # Batch size for each update\n",
        "n_epochs = 10              # Number of times to train on each batch\n",
        "gamma = 0.99               # Discount factor\n",
        "gae_lambda = 0.95          # GAE lambda, for variance reduction in advantage estimation\n",
        "clip_range = 0.2           # Clip range for PPO, helps with stable training\n",
        "\n",
        "# Instantiate PPO with custom hyperparameters\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",           # Policy type, can try \"CnnPolicy\" for image-based inputs\n",
        "    vec_env,\n",
        "    learning_rate=learning_rate,\n",
        "    n_steps=n_steps,\n",
        "    batch_size=batch_size,\n",
        "    n_epochs=n_epochs,\n",
        "    gamma=gamma,\n",
        "    gae_lambda=gae_lambda,\n",
        "    clip_range=clip_range,\n",
        "    verbose=1               # Verbose output\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "094qjCQoTEvo",
        "outputId": "4d8a779d-185c-49fc-a0ee-12e43de4019d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.63e+03 |\n",
            "|    ep_rew_mean     | -9.6e+05 |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 207       |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 25.478064 |\n",
            "|    clip_fraction        | 0.66      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.98     |\n",
            "|    explained_variance   | -6.08e-06 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.85e+05  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | 0.26      |\n",
            "|    value_loss           | 4.35e+07  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 203       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.73e-09 |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.01e+04  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -0.000606 |\n",
            "|    value_loss           | 5.41e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 207       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 39        |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.33e-11 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.69e+04  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 3.3e-06   |\n",
            "|    value_loss           | 3.77e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 206       |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 49        |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.02e-13 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.95e+03  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | 0.000224  |\n",
            "|    value_loss           | 2.12e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 202       |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 60        |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.08e-14 |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.33e+04  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -1.41e-05 |\n",
            "|    value_loss           | 1.14e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 201       |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 71        |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.85e-16 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 597       |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | 0.00179   |\n",
            "|    value_loss           | 8.88e+04  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 202       |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 81        |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.38e-17 |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.1e+03   |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | 2.4e-06   |\n",
            "|    value_loss           | 4.87e+04  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.63e+03  |\n",
            "|    ep_rew_mean          | -9.6e+05  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 201       |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 91        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.15e-17 |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.9e+03   |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | 0.000257  |\n",
            "|    value_loss           | 4.15e+04  |\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "3UzT_BZle8Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "vec_env2 = make_vec_env(MinesweeperEnvironment, n_envs=1, env_kwargs=dict(end_on_bomb=True))\n",
        "obs = vec_env2.reset()\n",
        "n_steps = 100\n",
        "total_substeps = 0\n",
        "for step in range(n_steps):\n",
        "    # action, _ = model.predict(obs, deterministic=False)\n",
        "\n",
        "    # Convert the observation to a tensor and ensure it's on the same device as the model\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(model.device)\n",
        "\n",
        "    # Get Q-values directly from the model's q_net\n",
        "    with torch.no_grad():\n",
        "        q_values = model.q_net(obs_tensor).cpu().numpy()\n",
        "\n",
        "    # Flatten the Q-values and get indices in descending order\n",
        "    ranked_actions = q_values[0].argsort()[::-1]\n",
        "\n",
        "    print(f\"found {len(ranked_actions)} ranked actions\")\n",
        "\n",
        "    for k, action_k in enumerate(ranked_actions):\n",
        "      total_substeps += 1\n",
        "      print(f\"trying action {k}\")\n",
        "      x = int(action_k) // 9\n",
        "      y = int(action_k) % 9\n",
        "      print(\"Action: \", (x, y), action_k)\n",
        "      obs, reward, done, info = vec_env2.step([action_k])\n",
        "      effect = info[0][\"effect\"]\n",
        "      print(\"reward=\", reward, \"done=\", done, \"effect=\", effect)\n",
        "      if not done:\n",
        "        vec_env2.render()\n",
        "\n",
        "      if done or (effect in [CLICK_GUESS, CLICK_VALID]):\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print(\"Won!\" if effect == GAME_WIN else \"Lost :(\")\n",
        "        print(\"total_steps:\", step + 1)\n",
        "        print(\"total_substeps:\", total_substeps)\n",
        "        break"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrxjJU58AYSP",
        "outputId": "0eb6001a-81b5-4f47-d4f5-c578098a181d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 4) 58\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * 1 * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 5) 14\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 71 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * 1 * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 5) 68\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 70 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * * 1 * * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 3) 57\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 69 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * 1 1 * * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 5) 59\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 68 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * * * \n",
            "6 * * * 1 1 1 * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (5, 2) 47\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 67 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * * 1 1 1 * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 2) 38\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 66 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * 1 * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * * 1 1 1 * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 1) 37\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 65 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * 1 1 * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * * 1 1 1 * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 2) 56\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 64 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * 1 1 * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * 1 1 1 1 * * * \n",
            "7 * * * * * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 3) 66\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 63 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * 1 1 * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * 1 1 1 1 * * * \n",
            "7 * * * 1 * 1 * * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 6) 69\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 62 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * 1 * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * 1 1 * * * * * * \n",
            "5 * * 1 * * * * * * \n",
            "6 * * 1 1 1 1 * * * \n",
            "7 * * * 1 * 1 1 * * \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 6) 60\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 25 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * * * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * 1 * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (3, 6) 33\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 24 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * * * 1 * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 1) 64\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 23 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 * * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) 72\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 22 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 * * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) 54\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 21 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 * * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 6) 15\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 20 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 * * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 7) 16\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 19 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 * 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 0) 36\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 18 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 * 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 1) 55\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * * * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) 77\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 * * * * 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 1) 73\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 1 * * * 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) 76\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 * * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 1 * * 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 6) 24\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 * * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 1 * * 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 7) 25\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 12 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 * 1 * 1 1 1 1 \n",
            "8 1 1 * * 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 2) 65\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 * * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (5, 0) 45\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 * \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 8) 26\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * * \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (3, 8) 35\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 6 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 * * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) 78\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * * * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 7) 7\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 * \n",
            "1 1 1 0 0 0 1 1 1 * \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 8) 8\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 2 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * * \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) 80\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 1 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 1 * 1 0 \n",
            "1 1 1 0 0 0 1 1 1 0 \n",
            "2 * 1 0 0 1 1 2 1 1 \n",
            "3 * 1 0 0 1 * 2 * 1 \n",
            "4 1 1 1 0 1 1 2 1 1 \n",
            "5 1 * 1 0 0 0 0 0 0 \n",
            "6 2 2 1 1 1 1 0 0 0 \n",
            "7 * 1 0 1 * 1 1 1 1 \n",
            "8 1 1 0 1 1 1 1 * 1 \n",
            "found 81 ranked actions\n",
            "trying action 0\n",
            "Action:  (3, 0) 27\n",
            "reward= [1.] done= [ True] effect= game-win\n",
            "Won!\n",
            "total_steps: 34\n",
            "total_substeps: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Minesweeper"
      ],
      "metadata": {
        "id": "AKuQeNVTmKio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play Random map minesweeper"
      ],
      "metadata": {
        "id": "M3tiK2GuuDTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_test = MinesweeperEnvironment(randomize_on_reset=False)\n",
        "\n",
        "print(\"Bombs before\")\n",
        "print(env_test.bombs)\n",
        "\n",
        "env_test.reset()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Bombs after\")\n",
        "print(env_test.bombs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i19am99FuGZc",
        "outputId": "b86f0f9a-e2c9-408b-f9cd-80ca95ee047b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bombs before\n",
            "[[0 0 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "\n",
            "Bombs after\n",
            "[[0 0 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO Solver"
      ],
      "metadata": {
        "id": "apdsy3bPuAVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "\n",
        "class CustomCNN2(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, features_dim=256):\n",
        "        super(CustomCNN2, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[2]  # Should be 2 for (9, 9, 2) input\n",
        "\n",
        "        # Define a custom CNN architecture\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the CNN dynamically\n",
        "        with th.no_grad():\n",
        "            sample_input = th.as_tensor(observation_space.sample()[None]).float().permute(0, 3, 1, 2)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "            print(\"Flattened output size after CNN:\", n_flatten)  # Debugging statement\n",
        "\n",
        "        # Define fully connected layers using the computed flattened size\n",
        "        # self.linear = nn.Sequential(\n",
        "        #     nn.Linear(n_flatten, 256),  # Use dynamically calculated n_flatten\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, 128),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, features_dim)  # Output size of features_dim (256)\n",
        "        # )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 1024),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(1024, 512),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(512, 256),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(1024, features_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        observations = observations.permute(0, 3, 1, 2)  # Rearrange dimensions for Conv2d\n",
        "        cnn_output = self.cnn(observations)\n",
        "        return self.linear(cnn_output)\n",
        "\n",
        "ppo_rewards = {\n",
        "    CLICK_VISIBLE: 0,\n",
        "    CLICK_BOMB: -2.,\n",
        "    GAME_WIN: 2,\n",
        "    CLICK_VALID: 0.5,\n",
        "    CLICK_GUESS: 0.25\n",
        "}\n",
        "\n",
        "vec_env_rand = make_vec_env(MinesweeperEnvironment, n_envs=1, env_kwargs=dict(end_on_bomb=True, randomize_on_reset=True, end_on_visible_click=False, rewards=ppo_rewards))\n",
        "\n",
        "# vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "# vec_env = VecTransposeImage(vec_env)  # Transpose to [batch_size, channels, height, width]\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN2,\n",
        "    features_extractor_kwargs=dict(features_dim=256)  # Output size of the final layer\n",
        ")\n",
        "\n",
        "# Define hyperparameters for PPO\n",
        "learning_rate = 1e-4       # Learning rate for PPO\n",
        "n_steps = 2048             # Number of steps to run for each environment per update\n",
        "batch_size = 64            # Batch size for each update\n",
        "n_epochs = 10              # Number of times to train on each batch\n",
        "gamma = 0.99               # Discount factor\n",
        "gae_lambda = 0.95          # GAE lambda, for variance reduction in advantage estimation\n",
        "clip_range = 0.2           # Clip range for PPO, helps with stable training\n",
        "\n",
        "# Instantiate PPO with custom hyperparameters\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",           # Policy type, can try \"CnnPolicy\" for image-based inputs\n",
        "    vec_env_rand,\n",
        "    learning_rate=learning_rate,\n",
        "    n_steps=n_steps,\n",
        "    batch_size=batch_size,\n",
        "    n_epochs=n_epochs,\n",
        "    gamma=gamma,\n",
        "    gae_lambda=gae_lambda,\n",
        "    clip_range=clip_range,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1               # Verbose output\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=300_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFBFyf6AmR6Q",
        "outputId": "91b9d713-3689-4f06-9f37-f7a873b06a83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Flattened output size after CNN: 2592\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 11.3     |\n",
            "|    ep_rew_mean     | -0.492   |\n",
            "| time/              |          |\n",
            "|    fps             | 461      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 11.1        |\n",
            "|    ep_rew_mean          | -0.542      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008461751 |\n",
            "|    clip_fraction        | 0.0359      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.39       |\n",
            "|    explained_variance   | -0.165      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.172       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    value_loss           | 0.438       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.5        |\n",
            "|    ep_rew_mean          | -0.525      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 355         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009146955 |\n",
            "|    clip_fraction        | 0.0743      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.38       |\n",
            "|    explained_variance   | 0.159       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0574      |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0308     |\n",
            "|    value_loss           | 0.258       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.8        |\n",
            "|    ep_rew_mean          | -0.615      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 349         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010853196 |\n",
            "|    clip_fraction        | 0.09        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.38       |\n",
            "|    explained_variance   | 0.17        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0268     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.038      |\n",
            "|    value_loss           | 0.156       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.4        |\n",
            "|    ep_rew_mean          | -0.655      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011278321 |\n",
            "|    clip_fraction        | 0.0937      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.37       |\n",
            "|    explained_variance   | 0.293       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0198     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0432     |\n",
            "|    value_loss           | 0.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.7        |\n",
            "|    ep_rew_mean          | -0.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 328         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011303142 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.37       |\n",
            "|    explained_variance   | 0.116       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0328      |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0429     |\n",
            "|    value_loss           | 0.148       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.48        |\n",
            "|    ep_rew_mean          | -0.595      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 324         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 44          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013632228 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.36       |\n",
            "|    explained_variance   | 0.265       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0226      |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.046      |\n",
            "|    value_loss           | 0.183       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 8.62        |\n",
            "|    ep_rew_mean          | -0.608      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 50          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016077463 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.36       |\n",
            "|    explained_variance   | 0.154       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0195      |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0532     |\n",
            "|    value_loss           | 0.165       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.11        |\n",
            "|    ep_rew_mean          | -0.74       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 322         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016183931 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.35       |\n",
            "|    explained_variance   | 0.234       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0951      |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0545     |\n",
            "|    value_loss           | 0.161       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.64        |\n",
            "|    ep_rew_mean          | -0.525      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 323         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 63          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017843105 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.35       |\n",
            "|    explained_variance   | 0.197       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0339     |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0577     |\n",
            "|    value_loss           | 0.149       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.85        |\n",
            "|    ep_rew_mean          | -0.588      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 321         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 70          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018577028 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.33       |\n",
            "|    explained_variance   | 0.28        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0506     |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0614     |\n",
            "|    value_loss           | 0.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.38        |\n",
            "|    ep_rew_mean          | -0.56       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 322         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 76          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020143207 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.32       |\n",
            "|    explained_variance   | 0.347       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0151     |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0603     |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 11.8        |\n",
            "|    ep_rew_mean          | -0.398      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021563862 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.31       |\n",
            "|    explained_variance   | 0.275       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0457     |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0626     |\n",
            "|    value_loss           | 0.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 11.2        |\n",
            "|    ep_rew_mean          | -0.575      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 321         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 89          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024521377 |\n",
            "|    clip_fraction        | 0.26        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.3        |\n",
            "|    explained_variance   | 0.263       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0152     |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.068      |\n",
            "|    value_loss           | 0.144       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.3        |\n",
            "|    ep_rew_mean          | -0.62       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 319         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 96          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022242652 |\n",
            "|    clip_fraction        | 0.27        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.31       |\n",
            "|    explained_variance   | 0.16        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0448     |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0656     |\n",
            "|    value_loss           | 0.117       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.1        |\n",
            "|    ep_rew_mean          | -0.605      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 102         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024247034 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.3        |\n",
            "|    explained_variance   | 0.314       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0768     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0657     |\n",
            "|    value_loss           | 0.137       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 10.2        |\n",
            "|    ep_rew_mean          | -0.458      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 319         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024235042 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.29       |\n",
            "|    explained_variance   | 0.239       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0649     |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0643     |\n",
            "|    value_loss           | 0.164       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 9.13        |\n",
            "|    ep_rew_mean          | -0.657      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 114         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026224785 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.28       |\n",
            "|    explained_variance   | 0.198       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0588     |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0655     |\n",
            "|    value_loss           | 0.173       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 9.21       |\n",
            "|    ep_rew_mean          | -0.733     |\n",
            "| time/                   |            |\n",
            "|    fps                  | 320        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 121        |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02902402 |\n",
            "|    clip_fraction        | 0.32       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.27      |\n",
            "|    explained_variance   | 0.221      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0116    |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0737    |\n",
            "|    value_loss           | 0.153      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 8.88        |\n",
            "|    ep_rew_mean          | -0.588      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 127         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026209183 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.25       |\n",
            "|    explained_variance   | 0.1         |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.068      |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0653     |\n",
            "|    value_loss           | 0.169       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 8.74        |\n",
            "|    ep_rew_mean          | -0.583      |\n",
            "| time/                   |             |\n",
            "|    fps                  | 319         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 134         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029985983 |\n",
            "|    clip_fraction        | 0.308       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.24       |\n",
            "|    explained_variance   | 0.242       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.0323      |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0693     |\n",
            "|    value_loss           | 0.191       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 11.9        |\n",
            "|    ep_rew_mean          | -0.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 320         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 140         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030208517 |\n",
            "|    clip_fraction        | 0.319       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.22       |\n",
            "|    explained_variance   | 0.181       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0733     |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0721     |\n",
            "|    value_loss           | 0.185       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 9.25      |\n",
            "|    ep_rew_mean          | -0.662    |\n",
            "| time/                   |           |\n",
            "|    fps                  | 319       |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 147       |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0286545 |\n",
            "|    clip_fraction        | 0.314     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.2      |\n",
            "|    explained_variance   | 0.292     |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | -0.048    |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -0.0723   |\n",
            "|    value_loss           | 0.116     |\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b612dc85b5c3>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mget_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0msample_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mprobs_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0msamples_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HER"
      ],
      "metadata": {
        "id": "EzY1I8RttmS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3 import PPO, A2C, DQN, HerReplayBuffer\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "\n",
        "class CustomCNN2(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, features_dim=256):\n",
        "        super(CustomCNN2, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[2]  # Should be 2 for (9, 9, 2) input\n",
        "\n",
        "        # Define a custom CNN architecture\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the CNN dynamically\n",
        "        with th.no_grad():\n",
        "            sample_input = th.as_tensor(observation_space.sample()[None]).float().permute(0, 3, 1, 2)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "            print(\"Flattened output size after CNN:\", n_flatten)  # Debugging statement\n",
        "\n",
        "        # Define fully connected layers using the computed flattened size\n",
        "        # self.linear = nn.Sequential(\n",
        "        #     nn.Linear(n_flatten, 256),  # Use dynamically calculated n_flatten\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, 128),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(256, features_dim)  # Output size of features_dim (256)\n",
        "        # )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 1024),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(1024, 512),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(512, 256),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(1024, features_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, observations):\n",
        "        observations = observations.permute(0, 3, 1, 2)  # Rearrange dimensions for Conv2d\n",
        "        cnn_output = self.cnn(observations)\n",
        "        return self.linear(cnn_output)\n",
        "\n",
        "ppo_rewards = {\n",
        "    CLICK_VISIBLE: 0,\n",
        "    CLICK_BOMB: -2.,\n",
        "    GAME_WIN: 2,\n",
        "    CLICK_VALID: 0.5,\n",
        "    CLICK_GUESS: 0.25\n",
        "}\n",
        "\n",
        "# vec_env_rand = make_vec_env(\n",
        "#     MinesweeperEnvironment,\n",
        "#     n_envs=1,\n",
        "#     env_kwargs=dict(\n",
        "#         end_on_bomb=True,\n",
        "#         randomize_on_reset=True,\n",
        "#         end_on_visible_click=False,\n",
        "#         rewards=ppo_rewards,\n",
        "#         use_dict_space=True\n",
        "#     )\n",
        "# )\n",
        "\n",
        "vec_env_rand = MinesweeperEnvironment(\n",
        "  end_on_bomb=True,\n",
        "  randomize_on_reset=True,\n",
        "  end_on_visible_click=False,\n",
        "  rewards=ppo_rewards,\n",
        "  use_dict_space=True\n",
        ")\n",
        "\n",
        "# vec_env = make_vec_env(MinesweeperEnvironment, n_envs=1)\n",
        "# vec_env = VecTransposeImage(vec_env)  # Transpose to [batch_size, channels, height, width]\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN2,\n",
        "    features_extractor_kwargs=dict(features_dim=256)  # Output size of the final layer\n",
        ")\n",
        "\n",
        "# Instantiate PPO with custom hyperparameters\n",
        "# model = PPO(\n",
        "#     \"CnnPolicy\",           # Policy type, can try \"CnnPolicy\" for image-based inputs\n",
        "#     vec_env_rand,\n",
        "#     learning_rate=learning_rate,\n",
        "#     n_steps=n_steps,\n",
        "#     batch_size=batch_size,\n",
        "#     n_epochs=n_epochs,\n",
        "#     gamma=gamma,\n",
        "#     gae_lambda=gae_lambda,\n",
        "#     clip_range=clip_range,\n",
        "#     policy_kwargs=policy_kwargs,\n",
        "#     replay_buffer_class=HerReplayBuffer,\n",
        "#     verbose=1               # Verbose output\n",
        "# )\n",
        "\n",
        "model = DQN(\n",
        "    \"MultiInputPolicy\",\n",
        "    vec_env_rand,\n",
        "    replay_buffer_class=HerReplayBuffer,\n",
        "    # Parameters for HER\n",
        "    replay_buffer_kwargs=dict(\n",
        "        n_sampled_goal=4,\n",
        "        goal_selection_strategy=\"future\",\n",
        "    ),\n",
        "    # policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=300_000)"
      ],
      "metadata": {
        "id": "Zuc7XnRGtlZ-",
        "outputId": "69002ade-17c7-4a8c-b57d-6bd9ad630dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 4.5      |\n",
            "|    ep_rew_mean      | -0.312   |\n",
            "|    exploration_rate | 0.999    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 3422     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 18       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 5.38     |\n",
            "|    ep_rew_mean      | -0.625   |\n",
            "|    exploration_rate | 0.999    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 2431     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 43       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 5.92     |\n",
            "|    ep_rew_mean      | -0.792   |\n",
            "|    exploration_rate | 0.998    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 2345     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 71       |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'achieved_goal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c08f70aa2fae>\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     ) -> SelfDQN:\n\u001b[0;32m--> 267\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Sample replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mreplay_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vec_normalize_env\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/her/her_replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, env)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_batch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_env_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Create virtual transitions by sampling new desired goals and computing new rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mvirtual_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_virtual_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirtual_batch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual_env_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Concatenate real and virtual data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/her/her_replay_buffer.py\u001b[0m in \u001b[0;36m_get_virtual_samples\u001b[0;34m(self, batch_indices, env_indices, env)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Sample and set new goals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mnew_goals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_goals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"desired_goal\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_goals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# The desired goal for the next observation must be the same as the previous one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/her/her_replay_buffer.py\u001b[0m in \u001b[0;36m_sample_goals\u001b[0;34m(self, batch_indices, env_indices)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mtransition_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransition_indices_in_episode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_ep_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"achieved_goal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransition_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtruncate_last_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'achieved_goal'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on random game"
      ],
      "metadata": {
        "id": "bRXMdnFuxkeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "vec_env2_rand = make_vec_env(MinesweeperEnvironment, n_envs=1, env_kwargs=dict(end_on_bomb=True, randomize_on_reset=True))\n",
        "obs = vec_env2_rand.reset()\n",
        "n_steps = 1000\n",
        "total_substeps = 0\n",
        "for step in range(n_steps):\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    ranked_actions = [action]\n",
        "\n",
        "    # # Convert the observation to a tensor and ensure it's on the same device as the model\n",
        "    # obs_tensor = torch.tensor(obs, dtype=torch.float32).to(model.device)\n",
        "\n",
        "    # # Get Q-values directly from the model's q_net\n",
        "    # with torch.no_grad():\n",
        "    #     q_values = model.q_net(obs_tensor).cpu().numpy()\n",
        "\n",
        "    # # Flatten the Q-values and get indices in descending order\n",
        "    # ranked_actions = q_values[0].argsort()[::-1]\n",
        "\n",
        "    print(f\"found {len(ranked_actions)} ranked actions\")\n",
        "\n",
        "    for k, action_k in enumerate(ranked_actions):\n",
        "      total_substeps += 1\n",
        "      print(f\"trying action {k}\")\n",
        "      x = int(action_k) // 9\n",
        "      y = int(action_k) % 9\n",
        "      print(\"Action: \", (x, y), action_k)\n",
        "      obs, reward, done, info = vec_env2_rand.step(action_k)\n",
        "      effect = info[0][\"effect\"]\n",
        "      print(\"reward=\", reward, \"done=\", done, \"effect=\", effect)\n",
        "      if not done:\n",
        "        vec_env2_rand.render()\n",
        "\n",
        "      if done or (effect in [CLICK_GUESS, CLICK_VALID]):\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print(\"Won!\" if effect == GAME_WIN else \"Lost :(\")\n",
        "        print(\"total_steps:\", step + 1)\n",
        "        print(\"total_substeps:\", total_substeps)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h34cCZFexjCm",
        "outputId": "f8d401c5-ee48-4b62-95a5-66f9f3c80cee"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (5, 7) [52]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 72 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * * * * * \n",
            "5 * * * * * * * 2 * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 5) [41]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 71 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 * * * * * * * * * \n",
            "1 * * * * * * * * * \n",
            "2 * * * * * * * * * \n",
            "3 * * * * * * * * * \n",
            "4 * * * * * 1 * * * \n",
            "5 * * * * * * * 2 * \n",
            "6 * * * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 8) [8]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 28 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * * 2 * \n",
            "6 2 3 * * * * * * * \n",
            "7 * * * * * * * * * \n",
            "8 * * * * * * * * * \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 8) [71]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 17 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 * * * * * * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 1) [73]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 16 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 * 2 * * * * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [0.3] done= [False] effect= click-valid\n",
            "Current Board: 15 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 * 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 1) [73]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 14 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * * 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 5) [59]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 1) [37]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 1) [10]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 13 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 * 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 12 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 4) [13]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 12 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * * * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 4) [67]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 11 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * * * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-eb198582e7e8>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  x = int(action_k) // 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 2) [65]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 10 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 10 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * * 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (5, 5) [50]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 4) [4]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 8) [62]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 2) [2]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 1) [37]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 0) [0]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 8) [80]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 8) [8]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 0) [0]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 8) [71]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 4) [22]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 9 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 * * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 2) [74]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 5) [14]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 8 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * * * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 3) [57]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 7 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 * 2 * 1 0 0 \n",
            "8 2 2 1 * 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 3) [75]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 0) [18]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 8) [62]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 3) [3]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 3) [21]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (3, 6) [33]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 0) [18]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 5) [23]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 0) [18]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 0) [0]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 2) [2]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 0) [0]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (1, 1) [10]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 8) [71]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 0) [18]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 0) [18]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 4) [4]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 0) [0]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 8) [62]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 8) [26]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (0, 6) [6]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 5 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * * \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (4, 8) [44]\n",
            "reward= [-0.3] done= [False] effect= click-guess\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 8) [62]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 6) [78]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (6, 0) [54]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 0) [72]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 3) [66]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (2, 8) [26]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 5) [77]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 3) [75]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 8) [71]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (8, 4) [76]\n",
            "reward= [-0.5] done= [False] effect= click-visible\n",
            "Current Board: 4 Cells Left\n",
            "# 0 1 2 3 4 5 6 7 8\n",
            "0 0 0 0 0 0 0 0 0 0 \n",
            "1 0 0 0 0 0 0 0 0 0 \n",
            "2 0 0 0 0 0 0 0 0 0 \n",
            "3 0 0 1 1 1 1 2 2 1 \n",
            "4 0 1 2 * * 1 * * 1 \n",
            "5 0 1 * * * 2 2 2 1 \n",
            "6 2 3 * 2 * 2 1 0 0 \n",
            "7 * * 1 1 2 * 1 0 0 \n",
            "8 2 2 1 0 1 1 1 0 0 \n",
            "found 1 ranked actions\n",
            "trying action 0\n",
            "Action:  (7, 1) [64]\n",
            "reward= [-1.] done= [ True] effect= click-bomb\n",
            "Lost :(\n",
            "total_steps: 157\n",
            "total_substeps: 157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old RL Zoo code"
      ],
      "metadata": {
        "id": "BM0Mu-fpw4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from rl_zoo3.train import train\n",
        "# from gym.envs.registration import register\n",
        "\n",
        "# register(\n",
        "#     id='Minesweeper-v1',\n",
        "#     entry_point='msenv:MinesweeperEnvironment',  # Update '__main__' to the module name if this is not in your main script\n",
        "#     max_episode_steps=100,  # Adjust based on expected game length\n",
        "# )\n",
        "\n",
        "# import gym\n",
        "# print([k for k in gym.envs.registry.keys() if \"Minesweeper\" in k])\n",
        "\n",
        "# !python -m rl_zoo3.train --algo dqn --env Minesweeper-v1 -f logs/ -c dqn.yml\n"
      ],
      "metadata": {
        "id": "DMBhu2n2sSky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6dqUaZx1mHnY"
      }
    }
  ]
}